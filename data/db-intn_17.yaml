- en: Chapter 14\. Consensus
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章 共识
- en: 'We’ve discussed quite a few concepts in distributed systems, starting with
    basics, such as links and processes, problems with distributed computing; then
    going through failure models, failure detectors, and leader election; discussed
    consistency models; and we’re finally ready to put it all together for a pinnacle
    of distributed systems research: distributed consensus.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在分布式系统中讨论了许多概念，从基础的如链接和进程开始，再到分布式计算的问题；然后讨论了故障模型、故障检测器和领导者选举；讨论了一致性模型；最后，我们准备把这一切整合到分布式系统研究的顶峰：分布式共识。
- en: Consensus algorithms in distributed systems allow multiple processes to reach
    an agreement on a value. FLP impossibility (see [“FLP Impossibility”](ch08.html#flp_impossibility))
    shows that it is impossible to guarantee consensus in a completely asynchronous
    system in a bounded time. Even if message delivery is guaranteed, it is impossible
    for one process to know whether the other one has crashed or is running slowly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，共识算法允许多个进程就某个值达成一致。FLP不可能性（参见[“FLP不可能性”](ch08.html#flp_impossibility)）表明，在完全异步系统中在有限时间内无法保证共识。即使消息传递被保证，一个进程也无法知道另一个进程是已经崩溃还是运行缓慢。
- en: In [Chapter 9](ch09.html#failure_detection), we discussed that there’s a trade-off
    between failure-detection accuracy and how quickly the failure can be detected.
    Consensus algorithms assume an asynchronous model and guarantee safety, while
    an external failure detector can provide information about other processes, guaranteeing
    liveness [[CHANDRA96]](app01.html#CHANDRA96). Since failure detection is not always
    fully accurate, there will be situations when a consensus algorithm waits for
    a process failure to be detected, or when the algorithm is restarted because some
    process is incorrectly suspected to be faulty.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](ch09.html#failure_detection)中，我们讨论了故障检测精度与故障快速检测之间的权衡。共识算法假设一个异步模型并保证安全性，而外部故障检测器可以提供关于其他进程的信息，保证活跃性[[CHANDRA96]](app01.html#CHANDRA96)。由于故障检测并不总是完全准确，所以在某些情况下，共识算法可能会等待检测到某个进程故障，或者因某个进程错误地被怀疑为有故障而重启算法。
- en: Processes have to agree on some value proposed by one of the participants, even
    if some of them happen to crash. A process is said to be *correct* if hasn’t crashed
    and continues executing algorithm steps. Consensus is extremely useful for putting
    events in a particular order, and ensuring consistency among the participants.
    Using consensus, we can have a system where processes move from one value to the
    next one without losing certainty about which values the clients observe.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 进程必须就由参与者之一提出的某个值达成一致，即使其中一些进程可能会崩溃。如果一个进程没有崩溃并且继续执行算法步骤，则称该进程为*正确*的。共识对于将事件放置在特定顺序中，并确保参与者之间的一致性非常有用。通过共识，我们可以构建一个系统，使得进程在不丢失客户端观察到的值的确定性的情况下，从一个值过渡到下一个值。
- en: 'From a theoretical perspective, consensus algorithms have three properties:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论的角度来看，共识算法具有三个属性：
- en: Agreement
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性
- en: The decision value is the same for all *correct* processes.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所有*正确*的进程对决策值都是相同的。
- en: Validity
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有效性
- en: The decided value was proposed by one of the processes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策的值由其中一个进程提出。
- en: Termination
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 终止
- en: All *correct* processes eventually reach the decision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有*正确*的进程最终会达成决策。
- en: 'Each one of these properties is extremely important. The agreement is embedded
    in the human understanding of consensus. The [dictionary definition of consensus](https://databass.dev/links/66)
    has the word “unanimity” in it. This means that upon the agreement, no process
    is allowed to have a different opinion about the outcome. Think of it as an agreement
    to meet at a particular time and place with your friends: all of you would like
    to meet, and only the specifics of the event are being agreed upon.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性中的每一个都非常重要。一致性是人们对共识的理解的基础。[共识的词典定义](https://databass.dev/links/66)中包含“一致性”一词。这意味着在达成一致意见后，没有任何一个过程可以对结果持不同意见。可以将其想象成与朋友们约定在特定的时间和地点会面：大家都愿意见面，只是具体的事件细节正在商定中。
- en: Validity is essential, because without it consensus can be trivial. Consensus
    algorithms require all processes to agree on some value. If processes use some
    predetermined, arbitrary default value as a decision output regardless of the
    proposed values, they will reach unanimity, but the output of such an algorithm
    will not be valid and it wouldn’t be useful in reality.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有效性是必不可少的，因为没有它共识就会变得微不足道。共识算法要求所有进程对某个值达成一致。如果进程使用某个预定的、任意的默认值作为输出决策，不考虑提议的值，它们将会达成一致，但这样的算法输出是无效的，现实中也没什么用。
- en: Without termination, our algorithm will continue forever without reaching any
    conclusion or will wait indefinitely for a crashed process to come back, which
    is not very useful, either. Processes have to agree eventually and, for a consensus
    algorithm to be practical, this has to happen rather quickly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有终止条件，我们的算法将会永远不会达成任何结论，或者会无限期地等待一个崩溃的进程回来，这也是不太有用的。进程最终必须达成一致，对于一个共识算法来说，这必须发生得相当迅速。
- en: Broadcast
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广播
- en: A *broadcast* is a communication abstraction often used in distributed systems.
    Broadcast algorithms are used to disseminate information among a set of processes.
    There exist many broadcast algorithms, making different assumptions and providing
    different guarantees. Broadcast is an important primitive and is used in many
    places, including consensus algorithms. We’ve discussed one of the forms of broadcast—gossip
    dissemination—already (see [“Gossip Dissemination”](ch12.html#gossip_dissipation)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*广播*是分布式系统中经常使用的通信抽象。广播算法用于在一组进程之间传播信息。存在许多广播算法，它们做出不同的假设并提供不同的保证。广播是一个重要的原语，在包括共识算法在内的许多地方都有应用。我们已经讨论了广播的一种形式——八卦传播——（见[“八卦传播”](ch12.html#gossip_dissipation)）。'
- en: 'Broadcasts are often used for database replication when a single coordinator
    node has to distribute the data to all other participants. However, making this
    process reliable is not a trivial matter: if the coordinator crashes after distributing
    the message to some nodes but not the other ones, it leaves the system in an inconsistent
    state: some of the nodes observe a new message and some do not.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 广播经常用于数据库复制，当单个协调节点必须将数据分发给所有其他参与者时。然而，使这个过程可靠并非易事：如果协调者在将消息分发给一些节点后崩溃，而没有分发给其他节点，系统就会处于不一致的状态：一些节点观察到了新消息，而其他节点没有。
- en: The simplest and the most straightforward way to broadcast messages is through
    a *best effort broadcast* [[CACHIN11]](app01.html#CACHIN11). In this case, the
    sender is responsible for ensuring message delivery to all the targets. If it
    fails, the other participants do not try to rebroadcast the message, and in the
    case of coordinator crash, this type of broadcast will fail silently.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 广播消息的最简单和最直接的方式是通过*尽力广播*[[CACHIN11]](app01.html#CACHIN11)。在这种情况下，发送者负责确保将消息传递给所有目标。如果发送失败，其他参与者不会尝试重新广播消息，而在协调者崩溃的情况下，这种类型的广播会静默地失败。
- en: For a broadcast to be *reliable*, it needs to guarantee that all correct processes
    receive the same messages, even if the sender crashes during transmission.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要使广播变得*可靠*，需要确保所有正确的进程在发送者在传输过程中崩溃时也能接收到相同的消息。
- en: To implement a naive version of a reliable broadcast, we can use a failure detector
    and a fallback mechanism. The most straightforward fallback mechanism is to allow
    every process that received the message to forward it to every other process it’s
    aware of. When the source process fails, other processes detect the failure and
    continue broadcasting the message, effectively *flooding* the network with `N²`
    messages (as shown in [Figure 14-1](#broadcast_2)). Even if the sender has crashed,
    messages still are picked up and delivered by the rest of the system, improving
    its reliability, and allowing all receivers to see the same messages [[CACHIN11]](app01.html#CACHIN11).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现一个可靠广播的天真版本，我们可以使用一个故障检测器和一个备用机制。最简单的备用机制是允许每个接收到消息的进程将其转发给它所知道的每个其他进程。当源进程失败时，其他进程会检测到故障并继续广播消息，有效地*淹没*网络，消息数量为`N²`（如图14-1所示）。即使发送者崩溃，消息仍然会被系统的其他部分接收和传递，提高其可靠性，使所有接收者能够看到相同的消息[[CACHIN11]](app01.html#CACHIN11)。
- en: '![dbin 1401](assets/dbin_1401.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1401](assets/dbin_1401.png)'
- en: Figure 14-1\. Broadcast
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1\. 广播
- en: One of the downsides of this approach is the fact that it uses `N²` messages,
    where `N` is the number of *remaining* recipients (since every broadcasting process
    excludes the original process and itself). Ideally, we’d want to reduce the number
    of messages required for a reliable broadcast.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法的一个缺点是它使用`N²`条消息，其中`N`是*剩余*收件人的数量（因为每个广播过程都排除原始进程和自身）。理想情况下，我们希望减少可靠广播所需的消息数量。
- en: Atomic Broadcast
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子广播
- en: Even though the flooding algorithm just described can ensure message delivery,
    it does not guarantee delivery in any particular order. Messages reach their destination
    eventually, at an unknown time. If we need to deliver messages in order, we have
    to use the *atomic broadcast* (also called the *total order multicast*), which
    guarantees both reliable delivery and total order.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 即使刚刚描述的泛洪算法可以确保消息的交付，但它不保证按任何特定顺序交付。消息最终会在未知时间到达目的地。如果我们需要按顺序交付消息，则必须使用*原子广播*（也称为*全序多播*），它保证了可靠交付和全序。
- en: While a reliable broadcast ensures that the processes agree on the set of messages
    delivered, an atomic broadcast also ensures they agree on the same sequence of
    messages (i.e., message delivery order is the same for every target).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可靠广播确保进程对交付的消息集达成一致意见，但原子广播还确保它们对消息序列的相同顺序达成一致（即每个目标的消息交付顺序相同）。
- en: 'In summary, an atomic broadcast has to ensure two essential properties:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，原子广播必须确保两个基本属性：
- en: Atomicity
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 原子性
- en: Processes have to agree on the set of received messages. Either all nonfailed
    processes deliver the message, or none do.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 进程必须就接收到的消息集达成一致意见。要么所有非故障进程交付消息，要么一个也不交付。
- en: Order
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序
- en: All nonfailed processes deliver the messages in the same order.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有非故障进程以相同顺序交付消息。
- en: 'Messages here are delivered *atomically*: every message is either delivered
    to all processes or none of them and, if the message is delivered, every other
    message is ordered before or after this message.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的消息被*原子地*交付：每条消息要么交付给所有进程，要么一个也不交付；如果消息被交付，那么每条其他消息都在此消息之前或之后排序。
- en: Virtual Synchrony
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 虚拟同步
- en: One of the frameworks for group communication using broadcast is called *virtual
    synchrony*. An atomic broadcast helps to deliver totally ordered messages to a
    *static* group of processes, and virtual synchrony delivers totally ordered messages
    to a *dynamic* group of peers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 用广播进行组通信的框架之一称为*虚拟同步*。原子广播有助于将完全有序的消息传递给*静态*进程组，而虚拟同步则将完全有序的消息传递给*动态*对等体组。
- en: Virtual synchrony organizes processes into groups. As long as the group exists,
    messages are delivered to all of its members in the same order. In this case,
    the order is not specified by the model, and some implementations can take this
    to their advantage for performance gains, as long as the order they provide is
    consistent across all members [[BIRMAN10]](app01.html#BIRMAN10).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟同步将进程组织成组。只要组存在，消息就会以相同顺序交付给其所有成员。在这种情况下，模型不指定顺序，一些实现可以利用这一点以提升性能，只要它们提供的顺序在所有成员之间保持一致[[BIRMAN10]](app01.html#BIRMAN10)。
- en: 'Processes have the same view of the group, and messages are associated with
    the group identity: processes can see the identical messages only as long as they
    belong to the same group.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 进程对组的视图相同，并且消息与组标识关联：只要它们属于同一组，进程可以看到相同的消息。
- en: As soon as one of the participants joins, leaves the group, or fails and is
    forced out of it, the group view changes. This happens by announcing the group
    change to all its members. Each message is uniquely associated with the group
    it has originated from.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦参与者之一加入、离开组或失败并被迫离开组，组视图就会改变。这是通过向所有成员宣布组更改来实现的。每条消息都与其来源的组唯一关联。
- en: Virtual synchrony distinguishes between the message *receipt* (when a group
    member receives the message) and its *delivery* (which happens when all the group
    members receive the message). If the message was *sent* in one view, it can be
    *delivered* only in the same view, which can be determined by comparing the current
    group with the group the message is associated with. Received messages remain
    pending in the queue until the process is notified about successful delivery.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟同步区分消息*接收*（当组成员接收消息时）和其*交付*（所有组成员接收消息时）。如果消息在一个视图中*发送*，则只能在相同视图中*交付*，这可以通过比较当前组与消息关联的组来确定。接收的消息在队列中保持未决，直到进程被通知成功交付为止。
- en: Since every message belongs to a specific group, unless all processes in the
    group have *received* it before the view change, no group member can consider
    this message *delivered*. This implies that all messages are sent and delivered
    *between* the view changes, which gives us atomic delivery guarantees. In this
    case, group views serve as a barrier that message broadcasts cannot pass.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每条消息都属于特定的组，除非组中所有进程在视图更改之前*接收*了它，否则任何组成员都不能认为此消息*已交付*。这意味着所有消息都在视图更改之间*发送*和*交付*，这为我们提供了原子交付保证。在这种情况下，组视图充当消息广播不能穿过的屏障。
- en: Some total broadcast algorithms order messages by using a single process (sequencer)
    that is responsible for determining it. Such algorithms can be easier to implement,
    but rely on detecting the leader failures for liveness. Using a sequencer can
    improve performance, since we do not need to establish consensus between processes
    for every message, and can use a sequencer-local view instead. This approach can
    still scale by partitioning the requests.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些总体广播算法通过使用负责确定的单一进程（顺序器）对消息进行排序。这样的算法实现起来更容易，但依赖于检测领导者故障以确保活性。使用顺序器可以提高性能，因为我们不需要为每条消息在进程之间建立一致性，可以使用本地顺序器视图。这种方法仍然可以通过对请求进行分区来扩展。
- en: Despite its technical soundness, virtual synchrony has not received broad adoption
    and isn’t commonly used in end-user commercial systems [[BIRMAN06]](app01.html#BIRMAN06).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管技术上很完备，虚拟同步并未得到广泛采用，在最终用户商业系统中也不常见 [[BIRMAN06]](app01.html#BIRMAN06)。
- en: Zookeeper Atomic Broadcast (ZAB)
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Zookeeper原子广播（ZAB）
- en: One of the most popular and widely known implementations of the atomic broadcast
    is ZAB used by [Apache Zookeeper](https://databass.dev/links/67) [[HUNT10]](app01.html#HUNT10)
    [[JUNQUEIRA11]](app01.html#JUNQUEIRA11), a hierarchical distributed key-value
    store, where it’s used to ensure the total order of events and atomic delivery
    necessary to maintain consistency between the replica states.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最广为人知和使用的原子广播实现之一是由[Apache Zookeeper](https://databass.dev/links/67)使用的ZAB [[HUNT10]](app01.html#HUNT10)
    [[JUNQUEIRA11]](app01.html#JUNQUEIRA11)，它是一个分层分布式键值存储系统，用于确保事件的完全顺序和原子传递，以维护副本状态之间的一致性。
- en: 'Processes in ZAB can take on one of two roles: *leader* and *follower*. Leader
    is a temporary role. It drives the process by executing algorithm steps, broadcasts
    messages to followers, and establishes the event order. To write new records and
    execute reads that observe the most recent values, clients connect to one of the
    nodes in the cluster. If the node happens to be a leader, it will handle the request.
    Otherwise, it forwards the request to the leader.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ZAB中的进程可以扮演两种角色之一：*领导者*和*追随者*。领导者是一个临时角色。它通过执行算法步骤推动流程，向追随者广播消息，并建立事件顺序。为了写入新记录并执行读取以观察最新值，客户端连接到集群中的一个节点。如果节点恰好是领导者，则处理请求。否则，将请求转发给领导者。
- en: To guarantee leader uniqueness, the protocol timeline is split into *epochs*,
    identified with a unique monotonically- and incrementally-sequenced number. During
    any epoch, there can be only one leader. The process starts from finding a *prospective
    leader* using any election algorithm, as long as it chooses a process that is
    up with a high probability. Since safety is guaranteed by the further algorithm
    steps, determining a prospective leader is more of a performance optimization.
    A prospective leader can also emerge as a consequence of the previous leader’s
    failure.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证领导者的唯一性，协议时间线被分割为*时期*，由唯一的单调递增序列号标识。在任何时期，只能有一个领导者。流程从使用任何选举算法找到一个*候选领导者*开始，只要它以高概率选择一个处于启动状态的进程。由于进一步的算法步骤保证了安全性，确定候选领导者更多地是性能优化。候选领导者也可能是前一领导者失败的结果。
- en: 'As soon as a prospective leader is established, it executes a protocol in three
    phases:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了候选领导者，它会执行三个阶段的协议：
- en: Discovery
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 发现
- en: The prospective leader learns about the latest epoch known by every other process,
    and proposes a new epoch that is *greater* than the current epoch of any follower.
    Followers respond to the epoch proposal with the identifier of the latest transaction
    seen in the previous epoch. After this step, no process will accept broadcast
    proposals for the earlier epochs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 候选领导者了解其他每个进程已知的最新时期，并提出一个新的大于*当前时期*的时期。追随者用前一时期中所见最新事务的标识符响应时期提议。此步骤后，没有进程将接受早期时期的广播提议。
- en: Synchronization
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同步
- en: This phase is used to recover from the previous leader’s failure and bring lagging
    followers up to speed. The prospective leader sends a message to the followers
    proposing itself as a leader for the new epoch and collects their acknowledgments.
    As soon as acknowledgments are received, the leader is established. After this
    step, followers will not accept attempts to become the epoch leader from any other
    processes. During synchronization, the new leader ensures that followers have
    the same history and delivers committed proposals from the established leaders
    of earlier epochs. These proposals are delivered *before* any proposal from the
    new epoch is delivered.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段用于从前一领导者的故障中恢复并使滞后的追随者跟上步伐。潜在领导者向追随者发送消息，提议自己作为新时期的领导者，并收集他们的确认。一旦收到确认，领导者就确立了。在此步骤之后，追随者将不再接受任何其他进程尝试成为时期领导者的请求。在同步期间，新领导者确保追随者具有相同的历史并传递早期时期已建立领导者的已承诺提议。这些提议在新时期的任何提议之前被传递*之前*。
- en: Broadcast
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 广播
- en: 'As soon as the followers are back in sync, active messaging starts. During
    this phase, the leader receives client messages, establishes their order, and
    broadcasts them to the followers: it sends a new proposal, waits for a quorum
    of followers to respond with acknowledgments and, finally, commits it. This process
    is similar to a two-phase commit without aborts: votes are just acknowledgments,
    and the client cannot vote against a valid leader’s proposal. However, proposals
    from the leaders from incorrect epochs should *not* be acknowledged. The broadcast
    phase continues until the leader crashes, is partitioned from the followers, or
    is suspected to be crashed due to the message delay.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦追随者恢复同步，活动消息传递就开始了。在此阶段，领导者接收客户端消息，建立它们的顺序，并广播给追随者：它发送一个新的提议，等待一定数量的追随者确认响应，最终进行提交。此过程类似于没有中止的两阶段提交：投票仅仅是确认，并且客户端无法反对有效领导者的提议。但不应确认来自不正确时期的领导者的提议。广播阶段将继续，直到领导者崩溃、与追随者分区或因消息延迟被怀疑崩溃。
- en: '[Figure 14-2](#zab_1) shows the three phases of the ZAB algorithm, and messages
    exchanged during each step.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-2](#zab_1) 显示了 ZAB 算法的三个阶段，以及在每个步骤期间交换的消息。'
- en: '![dbin 1402](assets/dbin_1402.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1402](assets/dbin_1402.png)'
- en: Figure 14-2\. ZAB protocol summary
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-2\. ZAB 协议概述
- en: The safety of this protocol is guaranteed if followers ensure they accept proposals
    only from the leader of the established epoch. Two processes may *attempt* to
    get elected, but only one of them can win and establish itself as an epoch leader.
    It is also assumed that processes perform the prescribed steps in good faith and
    follow the protocol.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果追随者确保仅接受来自已建立时期的领导者的提议，则该协议的安全性得到保证。两个进程可能*尝试*竞选，但只有一个可以获胜并确立自己为时期领导者。还假设进程忠实执行规定的步骤并遵循协议。
- en: Both the leader and followers rely on heartbeats to determine the liveness of
    the remote processes. If the leader does not receive heartbeats from the quorum
    of followers, it steps down as a leader, and restarts the election process. Similarly,
    if one of the followers has determined the leader crashed, it starts a new election
    process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者和追随者都依赖心跳来确定远程进程的活跃性。如果领导者未收到来自追随者的心跳，则会辞去领导者职务，并重新启动选举过程。同样，如果追随者之一确定领导者已崩溃，则会启动新的选举过程。
- en: Messages are totally ordered, and the leader will not attempt to send the next
    message until the message that preceded it was acknowledged. Even if some messages
    are received by a follower more than once, their repeated application do not produce
    additional side effects, as long as delivery order is followed. ZAB is able to
    handle multiple outstanding concurrent state changes from clients, since a unique
    leader will receive write requests, establish the event order, and broadcast the
    changes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 消息是完全有序的，领导者不会尝试发送下一条消息，直到前一条消息已被确认。即使某些消息被追随者接收多次，只要遵循交付顺序，它们的重复应用也不会产生额外的副作用。由于唯一领导者会接收写请求，建立事件顺序并广播更改，因此
    ZAB 能够处理客户端的多个并发状态更改。
- en: Total message order also allows ZAB to improve recovery efficiency. During the
    synchronization phase, followers respond with a highest committed proposal. The
    leader can simply choose the node with the highest proposal for recovery, and
    this can be the only node messages have to be copied from.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总消息顺序也使得 ZAB 能够提高恢复效率。在同步阶段，跟随者回复具有最高已提交提议。领导者可以简单地选择具有最高提议的节点进行恢复，这可能是消息需要从中复制的唯一节点。
- en: 'One of the advantages of ZAB is its efficiency: the broadcast process requires
    only two rounds of messages, and leader failures can be recovered from by streaming
    the missing messages from a single up-to-date process. Having a long-lived leader
    can have a positive impact on performance: we do not require additional consensus
    rounds to establish a history of events, since the leader can sequence them based
    on its local view.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ZAB 的一个优点是效率高：广播过程仅需要两轮消息，领导者失败可以通过从单个最新进程流式传输丢失的消息进行恢复。具有长期领导者可以对性能产生积极影响：我们不需要额外的共识轮来建立事件历史，因为领导者可以根据其本地视图对它们进行排序。
- en: Paxos
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Paxos
- en: An atomic broadcast is a problem equivalent to consensus in an asynchronous
    system with crash failures [[CHANDRA96]](app01.html#CHANDRA96), since participants
    have to *agree* on the message order and must be able to learn about it. You will
    see many similarities in both motivation and implementation between atomic broadcast
    and consensus algorithms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 原子广播是在具有崩溃失败的异步系统中等价于共识的问题，因为参与者必须*同意*消息顺序并且必须能够了解它。在原子广播和共识算法之间的动机和实现之间您将看到许多相似之处。
- en: Probably the most widely known consensus algorithm is *Paxos*. It was first
    introduced by Leslie Lamport in “The Part-Time Parliament” paper [[LAMPORT98]](app01.html#LAMPORT98).
    In this paper, consensus is described in terms of terminology inspired by the
    legislative and voting process on the Aegian island of Paxos. In 2001, the author
    released a follow-up paper titled “Paxos Made Simple” [[LAMPORT01]](app01.html#LAMPORT01)
    that introduced simpler terms, which are now commonly used to explain this algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是最广为人知的共识算法是*Paxos*。它最初由莱斯利·兰伯特在《兼职议会》论文中首次介绍。在这篇论文中，共识是用与帕克索斯岛上的立法和投票过程相关的术语来描述的。2001年，作者发布了一篇名为《简化版Paxos》的后续论文，引入了更简单的术语，现在通常用来解释这个算法。
- en: 'Participants in Paxos can take one of three roles: *proposers*, *acceptors*,
    or *learners*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Paxos 中的参与者可以扮演三种角色之一：*提议者*、*接受者*或*学习者*：
- en: Proposers
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 提议者
- en: Receive values from clients, create proposals to accept these values, and attempt
    to collect votes from acceptors.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户处接收值，创建提案以接受这些值，并尝试从接受者那里收集投票。
- en: Acceptors
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接受者
- en: Vote to accept or reject the values proposed by the proposer. For fault tolerance,
    the algorithm requires the presence of multiple acceptors, but for liveness, only
    a quorum (majority) of acceptor votes is required to accept the proposal.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 投票接受或拒绝提议者提出的值。对于容错性，算法需要多个接受者的存在，但对于活性，仅需要大多数接受者的选票来接受提议。
- en: Learners
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者
- en: Take the role of replicas, storing the outcomes of the accepted proposals.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 承担副本的角色，存储接受提议的结果。
- en: 'Any participant can take any role, and most implementations colocate them:
    a single process can simultaneously be a proposer, an acceptor, and a learner.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 任何参与者都可以扮演任何角色，并且大多数实现会将它们合并在一起：单个进程可以同时兼任提议者、接受者和学习者。
- en: Every proposal consists of a *value*, proposed by the client, and a unique monotonically
    increasing proposal number. This number is then used to ensure a total order of
    executed operations and establish happened-before/after relationships among them.
    Proposal numbers are often implemented using an `(id, timestamp)` pair, where
    node IDs are also comparable and can be used to break ties for timestamps.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提议包括客户提议的*值*和唯一的单调递增提议编号。然后使用这个编号来确保执行操作的总序，并建立它们之间的先后发生关系。提议编号通常使用 `(id,
    timestamp)` 对来实现，其中节点 ID 也是可比较的，并且可以用于打破时间戳的关系。
- en: Paxos Algorithm
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Paxos 算法
- en: 'The Paxos algorithm can be generally split into two phases: *voting* (or *propose*
    phase) and *replication*. During the voting phase, proposers compete to establish
    their leadership. During replication, the proposer distributes the value to the
    acceptors.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Paxos 算法通常可以分为两个阶段：*投票*（或*提议*阶段）和*复制*。在投票阶段，提议者竞争建立他们的领导地位。在复制阶段，提议者将值分发给接受者。
- en: The proposer is an initial point of contact for the client. It receives a value
    that should be decided upon, and attempts to collect votes from the quorum of
    acceptors. When this is done, acceptors distribute the information about the agreed
    value to the learners, ratifying the result. Learners increase the replication
    factor of the value that’s been agreed on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提议者是客户端的初始联系点。它接收应该决定的值，并尝试从接受者的法定人数中收集投票。完成后，接受者将关于已同意值的信息分发给学习者，批准结果。学习者增加了已同意的值的复制因子。
- en: Only one proposer can collect the majority of votes. Under some circumstances,
    votes may get split evenly between the proposers, and neither one of them will
    be able to collect a majority during this round, forcing them to restart. We discuss
    this and other scenarios of competing proposers in [“Failure Scenarios”](#paxos_failure_scenarios).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个提议者可以收集到多数票。在某些情况下，票数可能会均匀分配给提议者，而在这一轮中，没有一个提议者能够收集到多数票，迫使他们重新开始。我们在 [“失败场景”](#paxos_failure_scenarios)
    中讨论了这种以及其他竞争提议者的情况。
- en: During the propose phase, the *proposer* sends a `Prepare(n)` message (where
    `n` is a proposal number) to a majority of acceptors and attempts to collect their
    votes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在提议阶段，*提议者* 向大多数接受者发送 `Prepare(n)` 消息（其中 `n` 是提议编号），并尝试收集它们的投票。
- en: 'When the *acceptor* receives the prepare request, it has to respond, preserving
    the following invariants [[LAMPORT01]](app01.html#LAMPORT01):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *接受者* 收到准备请求时，必须响应，保留以下不变式 [[LAMPORT01]](app01.html#LAMPORT01)：
- en: If this acceptor hasn’t responded to a prepare request with a higher sequence
    number yet, it *promises* that it will not accept any proposal with a lower sequence
    number.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这个接受者还没有用更高的序列号响应准备请求，它*承诺*不会接受任何具有较低序列号的提议。
- en: If this acceptor has already accepted (received an `Accept!(m,v[accepted])`
    message) any other proposal earlier, it responds with a `Promise(m, v[accepted])`
    message, notifying the proposer that it has already accepted the proposal with
    a sequence number `m`.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这个接受者已经接受（收到 `Accept!(m,v[accepted])` 消息）任何其他早期提案，它将用 `Promise(m, v[accepted])`
    消息回复，通知提议者它已经接受了序列号为 `m` 的提案。
- en: If this acceptor has already responded to a prepare request with a higher sequence
    number, it notifies the proposer about the existence of a higher-numbered proposal.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这个接受者已经用更高序列号响应了准备请求，它会通知提议者存在一个更高序列号的提议。
- en: Acceptor can respond to more than one prepare request, as long as the later
    one has a higher sequence number .
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接受者可以对多个准备请求做出响应，只要后者具有更高的序列号。
- en: During the replication phase, after collecting a majority of votes, the *proposer*
    can start the replication, where it commits the proposal by sending acceptors
    an `Accept!(n, v)` message with value `v` and proposal number `n`. `v` is the
    value associated with the highest-numbered proposal among the responses it received
    from acceptors, or any value of its own if their responses did not contain old
    accepted proposals.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制阶段，收集到多数投票后，*提议者* 可以开始复制，通过发送接受者 `Accept!(n, v)` 消息，其中 `v` 是与从接受者收到的响应中最高编号的提案相关联的值，`n`
    是提案编号。如果响应中不包含旧的已接受提案，则可以使用任意自己的值 `v`。
- en: The *acceptor* accepts the proposal with a number `n`, unless during the propose
    phase it has already responded to `Prepare(m)`, where `m` is greater than `n`.
    If the acceptor rejects the proposal, it notifies the proposer about it by sending
    the highest sequence number it has seen along with the request to help the proposer
    catch up [[LAMPORT01]](app01.html#LAMPORT01).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*接受者* 接受具有编号 `n` 的提议，除非在提议阶段它已经对 `Prepare(m)` 做出了响应，其中 `m` 大于 `n`。如果接受者拒绝了提议，它会将它看到的最高序列号与请求一起发送给提议者，以帮助提议者赶上
    [[LAMPORT01]](app01.html#LAMPORT01)。'
- en: You can see a generalized depiction of a Paxos round in [Figure 14-3](#paxos_consensus_1).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 14-3](#paxos_consensus_1) 中可以看到 Paxos 轮次的泛化描述。
- en: '![dbin 1403](assets/dbin_1403.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1403](assets/dbin_1403.png)'
- en: 'Figure 14-3\. Paxos algorithm: normal execution'
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-3\. Paxos 算法：正常执行
- en: Once a consensus was reached on the value (in other words, it was accepted by
    at least one acceptor), future proposers have to decide on the same value to guarantee
    the agreement. This is why acceptors respond with the latest value they’ve accepted.
    If no acceptor has seen a previous value, the proposer is free to choose its own
    value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦就该值达成一致（换句话说，至少有一个接受者接受了它），未来的提议者必须决定同一个值以保证协议。这就是为什么接受者会回复他们接受的最新值。如果没有接受者看到先前的值，提议者可以自由选择它自己的值。
- en: A learner has to find out the value that has been decided, which it can know
    after receiving notification from the majority of acceptors. To let the learner
    know about the new value as soon as possible, acceptors can notify it about the
    value as soon as they accept it. If there’s more than one learner, each acceptor
    will have to notify each learner. One or more learners can be *distinguished*,
    in which case it will notify other learners about accepted values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者必须找出已经决定的值，它可以在接收到多数接受者通知后知道这个值。为了尽快让学习者了解新值，接受者可以在接受时立即通知它。如果有多个学习者，则每个接受者都必须通知每个学习者。一个或多个学习者可以被*区分*，在这种情况下，它将通知其他学习者接受的值。
- en: 'In summary, the goal of the first algorithm phase is to establish a leader
    for the round and understand which value is going to be accepted, allowing the
    leader to proceed with the second phase: broadcasting the value. For the purpose
    of the base algorithm, we assume that we have to perform both phases every time
    we’d like to decide on a value. In practice, we’d like to reduce the number of
    steps in the algorithm, so we allow the proposer to propose more than one value.
    We discuss this in more detail later in [“Multi-Paxos”](#multi_paxos).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，第一阶段算法的目标是确立本轮的领导者，并理解将要被接受的值，允许领导者继续第二阶段：广播该值。对于基本算法，我们假设每次决定一个值时都需要执行两个阶段。实际上，我们希望减少算法中的步骤数量，因此允许提议者提出多个值。我们稍后在[“多Paxos”](#multi_paxos)中会详细讨论这一点。
- en: Quorums in Paxos
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Paxos中的法定人数
- en: Quorums are used to make sure that *some* of the participants can fail, but
    we still can proceed as long as we can collect votes from the alive ones. A *quorum*
    is the *minimum* number of votes required for the operation to be performed. This
    number usually constitutes a *majority* of participants. The main idea behind
    quorums is that even if participants fail or happen to be separated by the network
    partition, there’s at least one participant that acts as an arbiter, ensuring
    protocol correctness.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用法定人数确保*某些*参与者可以失败，但只要我们能从存活的参与者收集投票，我们仍然可以继续进行。*法定人数*是执行操作所需的*最小*投票数。这个数字通常构成参与者的*多数*。法定人数背后的主要思想是，即使参与者失败或被网络分区隔离，至少有一个参与者充当仲裁者，确保协议的正确性。
- en: Once a sufficient number of participants accept the proposal, the value is guaranteed
    to be accepted by the protocol, since any two majorities have at least one participant
    in common.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦足够数量的参与者接受提案，协议保证该值会被接受，因为任意两个多数派至少有一个共同的参与者。
- en: Paxos guarantees safety in the presence of any number of failures. There’s no
    configuration that can produce incorrect or inconsistent states since this would
    contradict the definition of consensus.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Paxos在任何数量的故障情况下保证安全性。没有配置可以产生不正确或不一致的状态，因为这将违反共识的定义。
- en: Liveness is guaranteed in the presence of `f` failed processes. For that, the
    protocol requires `2f + 1` processes in total so that, if `f` processes happen
    to fail, there are still `f + 1` processes able to proceed. By using quorums,
    rather than requiring the presence of all processes, Paxos (and other consensus
    algorithms) guarantee results even when `f` process failures occur. In [“Flexible
    Paxos”](#flexible_paxos), we talk about quorums in slightly different terms and
    describe how to build protocols requiring quorum intersection between algorithm
    *steps* only.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在`f`个失败进程的情况下保证活性。为此，协议总共需要`2f + 1`个进程，以便如果`f`个进程恰好失败，仍然有`f + 1`个进程能够继续。通过使用法定人数而不是要求所有进程的存在，Paxos（以及其他共识算法）在`f`个进程失败时仍然能够保证结果。在[“灵活的Paxos”](#flexible_paxos)中，我们讨论了稍微不同术语下的法定人数，并描述了如何在算法*步骤*之间构建要求法定人数交集的协议。
- en: Tip
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'It is important to remember that quorums only describe the blocking properties
    of the system. To guarantee safety, for each step we have to wait for responses
    from *at least* a quorum of nodes. We can send proposals and accept commands to
    more nodes; we just do not have to wait for their responses to proceed. We may
    send messages to more nodes (some systems use *speculative execution*: issuing
    redundant queries that help to achieve the required response count in case of
    node failures), but to guarantee liveness, we can proceed as soon as we hear from
    the quorum.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，法定人数只描述系统的阻塞属性。为了确保安全，对于每一步，我们都必须等待至少一个法定人数的节点响应。我们可以向更多节点发送提议和接受命令；我们只是不必等待它们的响应即可继续。我们可以向更多节点发送消息（某些系统使用
    *推测执行*：发出冗余查询以帮助达到所需的响应计数，在节点故障时），但为了保证活性，我们可以在听到法定人数的响应后立即继续。
- en: Failure Scenarios
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 失败场景
- en: Discussing distributed algorithms gets particularly interesting when failures
    are discussed. One of the failure scenarios, demonstrating fault tolerance, is
    when the proposer fails during the second phase, before it is able to broadcast
    the value to all the acceptors (a similar situation can happen if the proposer
    is alive but is slow or cannot communicate with some acceptors). In this case,
    the new proposer may pick up and commit the value, distributing it to the other
    participants.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论分布式算法在讨论故障时尤为有趣。演示容错性的故障场景之一是当提议者在第二阶段失败之前无法向所有接收者广播值时（如果提议者仍然存活但速度较慢或无法与某些接收者通信时），新的提议者可以接替并提交值，将其分发给其他参与者。
- en: '[Figure 14-4](#paxos_failure_1) shows this situation:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-4](#paxos_failure_1) 展示了这种情况：'
- en: Proposer `P[1]` goes through the election phase with a proposal number `1`,
    but fails after sending the value `V1` to just one acceptor `A[1]`.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提议者 `P[1]` 以提议号 `1` 进入选举阶段，但仅向接收者 `A[1]` 发送值 `V1` 后失败。
- en: Another proposer `P[2]` starts a new round with a higher proposal number `2`,
    collects a quorum of acceptor responses (`A[1]` and `A[2]` in this case), and
    proceeds by committing the *old* value `V1`, proposed by `P[1]`.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个提议者 `P[2]` 以更高的提议号 `2` 开始新一轮，收集到接收者响应的法定人数（在这种情况下是 `A[1]` 和 `A[2]`），然后通过提交由
    `P[1]` 提出的 *旧* 值 `V1` 继续。
- en: '![dbin 1404](assets/dbin_1404.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1404](assets/dbin_1404.png)'
- en: 'Figure 14-4\. Paxos failure scenario: proposer failure, deciding on the old
    value'
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-4\. Paxos 失败场景：提议者失败，决定旧值
- en: Since the algorithm state is replicated to multiple nodes, proposer failure
    does not result in failure to reach a consensus. If the current proposer fails
    after even a single acceptor `A[1]` has accepted the value, its proposal *can*
    be picked by the next proposer. This also implies that all of it may happen without
    the original proposer knowing about it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法状态被复制到多个节点，提议者的失败不会导致无法达成共识。如果当前提议者在至少一个接收者 `A[1]` 接受值后失败，则其提议 *可以* 被下一个提议者选中。这也意味着所有这些可能发生，而原始提议者却不知情。
- en: In a client/server application, where the client is connected only to the original
    proposer, this might lead to situations where the client doesn’t know about the
    result of the Paxos round execution.^([1](ch14.html#idm46466885073000))
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端/服务器应用程序中，客户端仅连接到原始提议者的情况下，可能会导致客户端不知道 Paxos 轮执行的结果。^([1](ch14.html#idm46466885073000))
- en: 'However, other scenarios are possible, too, as [Figure 14-5](#paxos_failure_2)
    shows. For example:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有其他可能性，如 [图 14-5](#paxos_failure_2) 所示。例如：
- en: '`P[1]` has failed just like in the previous example, after sending the value
    `V1` only to `A[1]`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P[1]` 就像前面的例子一样失败，在仅向 `A[1]` 发送值 `V1` 后。'
- en: The next proposer, `P[2]`, starts a new round with a higher proposal number
    `2`, and collects a quorum of acceptor responses, but this time `A[2]` and `A[3]`
    are first to respond. After collecting a quorum, `P[2]` commits *its own value*
    despite the fact that theoretically there’s a different committed value on `A[1]`.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个提议者 `P[2]` 以较高的提议号 `2` 开始新一轮，并收集到接收者响应的法定人数，但这次 `A[2]` 和 `A[3]` 先响应。在收集到法定人数后，尽管理论上
    `A[1]` 上已经有不同的确定值，`P[2]` 仍然 *提交自己的值* 。
- en: '![dbin 1405](assets/dbin_1405.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1405](assets/dbin_1405.png)'
- en: 'Figure 14-5\. Paxos failure scenario: proposer failure, deciding on the new
    value'
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-5\. Paxos 失败场景：提议者失败，决定新值
- en: 'There’s one more possibility here, shown in [Figure 14-6](#paxos_failure_3):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一个可能性，如 [图 14-6](#paxos_failure_3) 所示：
- en: Proposer `P[1]` fails after only one acceptor `A[1]` accepts the value `V1`.
    `A[1]` fails shortly after accepting the proposal, before it can notify the next
    proposer about its value.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提议者 `P[1]` 在只有一个接受者 `A[1]` 接受值 `V1` 后失败。在接受提议后不久，`A[1]` 也失败了，在通知下一个提议者之前未能通知其值。
- en: Proposer `P[2]`, which started the round after `P[1]` failed, does not overlap
    with `A[1]` and proceeds to commit its value instead.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提议者 `P[2]` 在 `P[1]` 失败后启动了轮次，与 `A[1]` 没有重叠，并继续提交其值。
- en: Any proposer that comes *after* this round that will overlap with `A[1]`, will
    ignore `A[1]`’s value and choose a more recent accepted proposal instead.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何在此轮之后提出的提议者，如果与`A[1]`重叠，将会忽略`A[1]`的值，并选择一个更近期的已接受提议。
- en: '![dbin 1406](assets/dbin_1406.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1406](assets/dbin_1406.png)'
- en: 'Figure 14-6\. Paxos failure scenario: proposer failure, followed by the acceptor
    failure'
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-6\. Paxos 失败场景：提议者失败，随后是接受者失败
- en: Another failure scenario is when two or more proposers start competing, each
    trying to get through the propose phase, but keep failing to collect a majority
    because the other one beat them to it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个失败场景是，当两个或多个提议者开始竞争时，每个都试图在提议阶段通过，但由于对手抢先一步，他们都未能获得多数支持。
- en: While acceptors promise not to accept any proposals with a lower number, they
    still may respond to multiple prepare requests, as long as the later one has a
    higher sequence number. When a proposer tries to commit the value, it might find
    that acceptors have already responded to a prepare request with a higher sequence
    number. This may lead to multiple proposers constantly retrying and preventing
    each other from further progress. This problem is usually solved by incorporating
    a random backoff, which eventually lets one of the proposers proceed while the
    other one sleeps.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管接受者承诺不接受任何较低编号的提议，它们仍可能响应多个准备请求，只要后来的请求有更高的序列号。当一个提议者试图提交值时，可能会发现接受者已经对一个更高序列号的准备请求做出了响应。这可能导致多个提议者不断重试并阻止彼此进展。通常通过引入随机退避来解决这个问题，最终让其中一个提议者继续，而另一个则进入睡眠状态。
- en: The Paxos algorithm can tolerate acceptor failures, but only if there are still
    enough acceptors alive to form a majority.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Paxos 算法可以容忍接受者的故障，但前提是仍有足够的接受者存活以形成多数。
- en: Multi-Paxos
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多 Paxos
- en: 'So far we discussed the classic Paxos algorithm, where we pick an arbitrary
    proposer and attempt to start a Paxos round. One of the problems with this approach
    is that a propose round is required for each replication round that occurs in
    the system. Only after the proposer is established for the round, which happens
    after a majority of acceptors respond with a `Promise` to the proposer’s `Prepare`,
    can it start the replication. To avoid repeating the propose phase and let the
    proposer reuse its recognized position, we can use Multi-Paxos, which introduces
    the concept of a *leader*: a *distinguished* *proposer* [[LAMPORT01]](app01.html#LAMPORT01).
    This is a crucial addition, significantly improving algorithm efficiency.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了经典的 Paxos 算法，其中我们选择一个任意的提议者并尝试启动一个 Paxos 轮。这种方法的一个问题是，在系统中每次复制轮次都需要一个提议轮次。只有在提议者为该轮次建立之后（这发生在大多数接受者对提议者的
    `Prepare` 做出 `Promise` 响应后），它才能开始复制。为了避免重复提议阶段并让提议者重复使用其已识别的位置，我们可以使用多 Paxos，引入了“领导者”的概念：一个“杰出的提议者”[[LAMPORT01]](app01.html#LAMPORT01)。这是一个重要的增强，显著提高了算法的效率。
- en: 'Having an established leader, we can skip the propose phase and proceed straight
    to replication: distributing a value and collecting acceptor acknowledgments.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个确立的领导者，我们可以跳过提议阶段，直接进行复制：分发一个值并收集接受者的确认。
- en: In the classic Paxos algorithm, reads can be implemented by running a Paxos
    round that would collect any values from incomplete rounds if they’re present.
    This has to be done because the last known proposer is not guaranteed to hold
    the most recent data, since there might have been a different proposer that has
    modified state without the proposer knowing about it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的 Paxos 算法中，可以通过运行一个 Paxos 轮来实现读操作，该轮将收集任何不完整轮次中的值（如果存在）。必须这样做是因为最后已知的提议者不能保证持有最新的数据，因为可能已经有其他提议者修改了状态，而不让当前提议者知道。
- en: 'A similar situation may occur in Multi-Paxos: we’re trying to perform a read
    from the known leader *after* the other leader is already elected, returning stale
    data, which contradicts the linearizability guarantees of consensus. To avoid
    that and guarantee that no other process can successfully submit values, some
    Multi-Paxos implementations use *leases*. The leader periodically contacts the
    participants, notifying them that it is still alive, effectively prolonging its
    lease. Participants have to respond and allow the leader to continue operation,
    promising that they will not accept proposals from other leaders for the period
    of the lease [[CHANDRA07]](app01.html#CHANDRA07).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在多 Paxos 中可能会发生类似的情况：我们试图在已知领导者*之后*从另一个已选举的领导者那里执行读取，返回旧数据，这与一致性保证相矛盾。为了避免这种情况，并保证没有其他进程能够成功提交值，一些多
    Paxos 实现使用*租约*。领导者定期联系参与者，通知他们自己仍然活跃，有效延长其租期。参与者必须响应并允许领导者继续操作，并承诺在租约期内不接受其他领导者的提议
    [[CHANDRA07]](app01.html#CHANDRA07)。
- en: Leases are not a correctness guarantee, but a performance optimization that
    allows reads from the active leader without collecting a quorum. To guarantee
    safety, leases rely on the bounded clock synchrony between the participants. If
    their clocks drift too much and the leader assumes its lease is still valid while
    other participants think its lease has expired, linearizability *cannot* be guaranteed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 租约不是正确性保证，而是允许从活动领导者进行读取而不收集法定人数的性能优化。为了保证安全性，租约依赖于参与者之间有界的时钟同步。如果他们的时钟漂移太大，而领导者认为其租约仍然有效，而其他参与者认为其租约已经过期，则无法保证一致性。
- en: Multi-Paxos is sometimes described as a *replicated log* of operations applied
    to some structure. The algorithm is oblivious to the semantics of this structure
    and is only concerned with consistently replicating values that will be appended
    to this log. To preserve the state in case of process crashes, participants keep
    a durable log of received messages.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有时将多 Paxos 描述为应用于某些结构的*复制日志*的算法。该算法对这个结构的语义视而不见，只关注一致地复制将追加到此日志中的值。为了在进程崩溃时保留状态，参与者保持接收消息的持久日志。
- en: To prevent a log from growing indefinitely large, its contents should be applied
    to the aforementioned structure. After the log contents are synchronized with
    a primary structure, creating a snapshot, the log can be truncated. Log and state
    snapshots should be mutually consistent, and snapshot changes should be applied
    atomically with truncation of the log segment [[CHANDRA07]](app01.html#CHANDRA07).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止日志无限增长，其内容应用于上述结构。在日志内容与主要结构同步并创建快照后，可以截断日志。日志和状态快照应该互相一致，并且快照变化应该与日志段截断同时进行
    [[CHANDRA07]](app01.html#CHANDRA07)。
- en: 'We can think of single-decree Paxos as a *write-once register*: we have a slot
    where we can put a value, and as soon as we’ve written the value there, no subsequent
    modifications are possible. During the first step, proposers compete for ownership
    of the register, and during the second phase, one of them writes the value. At
    the same time, Multi-Paxos can be thought of as an append-only log, consisting
    of a sequence of such values: we can write one value at a time, all values are
    strictly ordered, and we cannot modify already written values [[RYSTSOV16]](app01.html#RYSTSOV16).
    There are examples of consensus algorithms that offer collections of read-modify-write
    registers and use state sharing rather than replicated state machines, such as
    Active Disk Paxos [[CHOCKLER15]](app01.html#CHOCKLER15) and CASPaxos [[RYSTSOV18]](app01.html#RYSTSOV18).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将单决策 Paxos 理解为*一次写寄存器*：我们有一个槽位可以放置一个值，一旦我们在那里写入了值，就不能再进行后续修改。在第一步中，提议者竞争注册表的所有权，在第二阶段中，其中一个写入值。同时，多
    Paxos 可以被视为一个追加日志，由这些值的序列组成：我们一次只能写入一个值，所有值严格有序，并且不能修改已经写入的值 [[RYSTSOV16]](app01.html#RYSTSOV16)。有些一致性算法的示例提供了集合读取修改写寄存器并使用状态共享而不是复制状态机，例如
    Active Disk Paxos [[CHOCKLER15]](app01.html#CHOCKLER15) 和 CASPaxos [[RYSTSOV18]](app01.html#RYSTSOV18)。
- en: Fast Paxos
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速 Paxos
- en: We can reduce the number of round-trips by one, compared to the classic Paxos
    algorithm, by letting *any* proposer contact acceptors directly rather than going
    through the leader. For this, we need to increase the quorum size to `2f + 1`
    (where `f` is the number of processes allowed to fail), compared to `f + 1` in
    classic Paxos, and a total number of acceptors to `3f + 1` [[JUNQUEIRA07]](app01.html#JUNQUEIRA07).
    This optimization is called *Fast Paxos* [[LAMPORT06]](app01.html#LAMPORT06).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于经典 Paxos 算法，我们可以通过让*任何*提议者直接联系接受者而不是通过领导者，减少一次往返。为此，我们需要将仲裁组大小增加到`2f + 1`（其中`f`是允许失败的进程数），与经典
    Paxos 的`f + 1`相比，以及总接受者数量增加到`3f + 1` [[JUNQUEIRA07]](app01.html#JUNQUEIRA07)。这种优化称为*快速
    Paxos* [[LAMPORT06]](app01.html#LAMPORT06)。
- en: 'The classic Paxos algorithm has a condition, where during the replication phase,
    the proposer can pick any value it has collected during the propose phase. Fast
    Paxos has two types of rounds: *classic*, where the algorithm proceeds the same
    way as the classic version, and *fast*, where it allows acceptors to accept other
    values.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 经典 Paxos 算法有一个条件，在复制阶段，提议者可以选择其在提议阶段收集到的任何值。快速 Paxos 有两种类型的轮次：*经典*，其中算法与经典版本相同，和*快速*，其中允许接受者接受其他值。
- en: While describing this algorithm, we will refer to the proposer that has collected
    a sufficient number of responses during the propose phase as a *coordinator*,
    and reserve term *proposer* for all other proposers. Some Fast Paxos descriptions
    say that *clients* can contact acceptors directly [[ZHAO15]](app01.html#ZHAO15).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述该算法时，我们将那些在提议阶段收集到足够数量响应的提议者称为*协调者*，并保留术语*提议者*用于所有其他提议者。一些快速 Paxos 的描述称*客户*可以直接联系接受者
    [[ZHAO15]](app01.html#ZHAO15)。
- en: In a fast round, if the coordinator is permitted to pick its own value during
    the replication phase, it can instead issue a special `Any` message to acceptors.
    Acceptors, in this case, are allowed to treat *any* proposer’s value as if it
    is a classic round and they received a message with this value from the coordinator.
    In other words, acceptors independently decide on values they receive from different
    proposers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速轮次中，如果协调者在复制阶段被允许选择自己的值，它可以发出一个特殊的`Any`消息给接受者。在这种情况下，接受者可以将*任何*提议者的值视为经典轮次，并认为它们从协调者接收到这个值的消息。换句话说，接受者可以独立地决定它们从不同提议者那里收到的值。
- en: '[Figure 14-7](#fast_paxos_1) shows an example of classic and fast rounds in
    Fast Paxos. From the image it might look like the fast round has more execution
    steps, but keep in mind that in a classic round, in order to submit its value,
    the proposer would need to go through the coordinator to get its value committed.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-7](#fast_paxos_1) 显示了快速 Paxos 中经典和快速轮次的示例。从图像中可以看出，快速轮次可能具有更多的执行步骤，但请记住，在经典轮次中，为了提交其值，提议者需要通过协调者确保其值已提交。'
- en: '![dbin 1407](assets/dbin_1407.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1407](assets/dbin_1407.png)'
- en: 'Figure 14-7\. Fast Paxos algorithm: fast and classic rounds'
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-7\. 快速 Paxos 算法：快速和经典轮次
- en: This algorithm is prone to *collisions*, which occur if two or more proposers
    attempt to use the *fast* step and reduce the number of round-trips, and acceptors
    receive different values. The coordinator has to intervene and start recovery
    by initiating a new round.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个或更多的提议者尝试使用*快速*步骤并减少往返次数，且接受者接收到不同的值，则该算法容易发生*冲突*。协调者必须介入并通过启动新的轮次来开始恢复。
- en: This means that acceptors, after receiving values from different proposers,
    may decide on conflicting values. When the coordinator detects a conflict (value
    collision), it has to reinitiate a `Propose` phase to let acceptors converge to
    a single value.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着接受者在从不同的提议者那里收到值后，可能会决定使用冲突的值。当协调者检测到冲突（值冲突）时，必须重新启动`Propose`阶段，以使接受者收敛到单一值。
- en: One of the disadvantages of Fast Paxos is the increased number of round-trips
    and request latency on collisions if the request rate is high. [[JUNQUEIRA07]](app01.html#JUNQUEIRA07)
    shows that, due to the increased number of replicas and, subsequently, *messages*
    exchanged between the participants, despite a reduced number of steps, Fast Paxos
    can have higher latencies than its classic counterpart.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 快速 Paxos 的一个缺点是如果请求速率高时，由于碰撞会增加往返次数和请求的延迟。[[JUNQUEIRA07]](app01.html#JUNQUEIRA07)
    表明，由于增加了副本的数量，随后增加了参与者之间交换的*消息*数量，尽管步骤减少了，快速 Paxos 的延迟可能比其经典对应版本更高。
- en: Egalitarian Paxos
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平等 Paxos
- en: 'Using a distinguished proposer as a leader makes a system prone to failures:
    as soon as the leader fails, the system has to elect a new one before it can proceed
    with further steps. Another problem is that having a leader can put a disproportionate
    load on it, impairing system performance.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个特定的提议者作为领导者会使系统容易发生故障：一旦领导者失败，系统必须在继续下一步之前选举一个新的领导者。另一个问题是，有一个领导者可能会给它带来不成比例的负载，影响系统性能。
- en: Note
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One of the ways to avoid putting an entire system load on the leader is *partitioning*.
    Many systems split the range of possible values into smaller segments and allow
    a part of the system to be responsible for a specific range without having to
    worry about the other parts. This helps with availability (by isolating failures
    to a single partition and preventing propagation to other parts of the system),
    performance (since segments serving different values are nonoverlapping), and
    scalability (since we can scale the system by increasing the number of partitions).
    It is important to keep in mind that performing an operation against *multiple*
    partitions will require an atomic commitment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 避免将整个系统负载放在领导者上的一种方法是 *分区*。许多系统将可能值的范围分割成较小的段，并允许系统的一部分负责特定范围，而不必担心其他部分。这有助于提高可用性（通过将故障隔离到单个分区并防止传播到系统的其他部分），性能（因为服务不同值的段是不重叠的），以及可扩展性（通过增加分区数可以扩展系统）。重要的是要记住，对
    *多个* 分区执行操作将需要原子提交。
- en: Instead of using a leader and proposal numbers for sequencing commands, we can
    use a leader responsible for the commit of the *specific* command, and establish
    the order by looking up and setting dependencies. This approach is commonly called
    Egalitarian Paxos, or EPaxos [[MORARU11]](app01.html#MORARU11). The idea of allowing
    nonconflicting writes to be committed to the replicated state machine independently
    was first introduced in [[LAMPORT05]](app01.html#LAMPORT05) and called Generalized
    Paxos. EPaxos is a first implementation of Generalized Paxos.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用领导者和提案号为命令排序不同，我们可以使用一个负责特定命令提交的领导者，并通过查找和设置依赖项来建立顺序。这种方法通常称为平等 Paxos 或 EPaxos
    [[MORARU11]](app01.html#MORARU11)。允许非冲突写入独立提交到复制状态机的想法最早在 [[LAMPORT05]](app01.html#LAMPORT05)
    中引入，并称为广义 Paxos。EPaxos 是广义 Paxos 的首次实现。
- en: EPaxos attempts to offer benefits of both the classic Paxos algorithm and Multi-Paxos.
    Classic Paxos offers high availability, since a leader is established during each
    round, but has a higher message complexity. Multi-Paxos offers high throughput
    and requires fewer messages, but a leader may become a bottleneck.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: EPaxos 试图提供经典 Paxos 算法和多 Paxos 的双重好处。经典 Paxos 提供高可用性，因为每轮都会建立一个领导者，但消息复杂度较高。多
    Paxos 提供高吞吐量，并且需要较少的消息，但领导者可能成为瓶颈。
- en: 'EPaxos starts with a *Pre-Accept* phase, during which a process becomes a leader
    for the specific proposal. Every proposal has to include:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: EPaxos 从 *预接受* 阶段开始，此时一个进程成为特定提案的领导者。每个提案都必须包括：
- en: Dependencies
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖关系
- en: All commands that potentially interfere with a current proposal, but are not
    necessarily already committed.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可能干扰当前提案但尚未必然已经提交的命令。
- en: A sequence number
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 序列号
- en: This breaks cycles between the dependencies. Set it with a value larger than
    any sequence number of the known dependencies.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这打破了依赖之间的循环。将其设置为大于已知依赖的任何序列号的值。
- en: After collecting this information, it forwards a `Pre-Accept` message to a *fast
    quorum* of replicas. A fast quorum is `⌈3f/4⌉` replicas, where `f` is the number
    of tolerated failures.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 收集此信息后，它向 *快速法定复制品* 发送 `预接受` 消息。快速法定复制品是 `⌈3f/4⌉` 个复制品，其中 `f` 是可以容忍的故障数。
- en: Replicas check their local command logs, update the proposal dependencies based
    on their view of potentially conflicting proposals, and send this information
    back to the leader. If the leader receives responses from a fast quorum of replicas,
    and their dependency lists are in agreement with each other and the leader itself,
    it can commit the command.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 复制品检查其本地命令日志，根据它们对潜在冲突提案的观点更新提案依赖，并将此信息发送回领导者。如果领导者从快速法定复制品收到响应，并且它们的依赖列表在彼此和领导者本身之间达成一致，它可以提交该命令。
- en: If the leader does not receive enough responses or if the command lists received
    from the replicas differ and contain interfering commands, it updates its proposal
    with a new dependency list and a sequence number. The new dependency list is based
    on previous replica responses and combines *all* collected dependencies. The new
    sequence number has to be larger than the highest sequence number seen by the
    replicas. After that, the leader sends the new, updated command to `⌊f/2⌋ + 1`
    replicas. After this is done, the leader can finally commit the proposal.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果领导者未收到足够的响应，或者如果来自副本的命令列表不同并包含干扰命令，则它将使用新的依赖关系列表和序列号更新其提案。新的依赖关系列表基于之前的副本响应并合并*所有*收集的依赖关系。新的序列号必须大于副本所见到的最高序列号。之后，领导者将更新的命令发送给`⌊f/2⌋
    + 1`个副本。完成此步骤后，领导者最终可以提交提案。
- en: 'Effectively, we have two possible scenarios:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们有两种可能的情况：
- en: Fast path
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 快速路径
- en: When dependencies match and the leader can safely proceed with the commit phase
    with only a *fast quorum* of replicas.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当依赖关系匹配且领导者可以安全地继续使用仅快速法定副本提交阶段时。
- en: Slow path
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 慢速路径
- en: When there’s a disagreement between the replicas, and their command lists have
    to be updated before the leader can proceed with a commit.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当副本之间存在分歧并且它们的命令列表必须在领导者可以继续提交之前更新时。
- en: '[Figure 14-8](#epaxos_1) shows these scenarios—`P[1]` initiating a fast path
    run, and `P[5]` initiating a slow path run:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-8](#epaxos_1) 展示了这些场景——`P[1]` 发起快速路径运行，`P[5]` 发起慢速路径运行：'
- en: '`P[1]` starts with proposal number `1` and no dependencies, and sends a `PreAccept(1,
    ∅)` message. Since the command logs of `P[2]` and `P[3]` are empty, `P[1]` can
    proceed with a commit.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P[1]` 使用提案编号 `1` 并没有依赖关系，发送 `PreAccept(1, ∅)` 消息。由于 `P[2]` 和 `P[3]` 的命令日志为空，`P[1]`
    可以继续提交。'
- en: '`P[5]` creates a proposal with sequence number `2`. Since its command log is
    empty by that point, it also declares no dependencies and sends a `PreAccept(2,
    ∅)` message. `P[4]` is not aware of the committed proposal `1`, but `P[3]` notifies
    `P[5]` about the conflict and sends its command log: `{1}`.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P[5]` 创建带有序列号 `2` 的提案。由于到达这一点时其命令日志为空，它也声明没有依赖关系，并发送 `PreAccept(2, ∅)` 消息。`P[4]`
    不知道已提交的提案 `1`，但 `P[3]` 通知 `P[5]` 存在冲突，并发送其命令日志：`{1}`。'
- en: '`P[5]` updates its local dependency list and sends a message to make sure replicas
    have the same dependencies: `Accept(2,{1})`. As soon as the replicas respond,
    it can commit the value.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P[5]` 更新其本地依赖列表，并发送消息以确保副本具有相同的依赖关系：`Accept(2,{1})`。一旦副本响应，它就可以提交该值。'
- en: '![dbin 1408](assets/dbin_1408.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1408](assets/dbin_1408.png)'
- en: Figure 14-8\. EPaxos algorithm run
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-8\. EPaxos 算法运行
- en: Two commands, `A` and `B`, interfere only if their execution order matters;
    in other words, if executing `A` before `B` and executing `B` before `A` produce
    different results.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 两个命令 `A` 和 `B` 只有在它们的执行顺序有所影响时才会干扰；换句话说，如果在执行 `A` 之前执行 `B` 和在执行 `B` 之前执行 `A`
    会产生不同的结果。
- en: Commit is done by responding to the client and asynchronously notifying replicas
    with a `Commit` message. Commands are executed *after* they’re committed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 提交通过向客户端响应并异步通知副本发送`Commit`消息完成。命令在其提交之后才会执行。
- en: 'Since dependencies are collected during the Pre-Accept phase, by the time requests
    are executed, the command order is already established and no command can suddenly
    appear somewhere in-between: it can only get appended *after* the command with
    the largest sequence number.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在预接受阶段收集了依赖关系，在执行请求时，命令顺序已经确定，没有命令会突然出现在中间位置：只有在最大序列号的命令之后才会追加。
- en: To execute a command, replicas build a dependency graph and execute all commands
    in a reverse dependency order. In other words, before a command can be executed,
    all its dependencies (and, subsequently, all their dependencies) have to be executed.
    Since only interfering commands have to depend on each other, this situation should
    be relatively rare for most workloads [[MORARU13]](app01.html#MORARU13).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行命令，副本构建依赖图，并按照反向依赖顺序执行所有命令。换句话说，在执行一个命令之前，必须执行其所有依赖项（以及随后的所有依赖项）。由于只有干扰命令需要相互依赖，对于大多数工作负载来说，这种情况应该相对罕见[[MORARU13]](app01.html#MORARU13)。
- en: Similar to Paxos, EPaxos uses proposal numbers, which prevent stale messages
    from being propagated. Sequence numbers consist of an *epoch* (identifier of the
    current cluster configuration that changes when nodes leave and join the cluster),
    a monotonically incremented node-local counter, and a replica ID. If a replica
    receives a proposal with a sequence number lower than one it has already seen,
    it negatively acknowledges the proposal, and sends the highest sequence number
    and an updated command list known to it in response.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Paxos，EPaxos 使用提案编号，防止传播过期消息。序列号由*时期*（当前集群配置的标识符，在节点离开和加入集群时更改）、单调递增的节点本地计数器和副本
    ID 组成。如果一个副本收到一个比其已见过的序列号更低的提议，它将否定该提议，并发送其响应中已知的最高序列号和更新的命令列表。
- en: Flexible Paxos
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灵活 Paxos
- en: 'A quorum is usually defined as a majority of processes. By definition, we have
    an *intersection* between two quorums no matter how we pick nodes: there’s always
    at least one node that can break ties.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通常将法定人数定义为多数进程。根据定义，无论我们如何选择节点，两个法定人数之间始终存在*交集*：总有至少一个节点可以打破僵局。
- en: 'We have to answer two important questions:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须回答两个重要问题：
- en: Is it necessary to contact the *majority* of servers during *every* execution
    step?
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*每个*执行步骤期间是否需要联系*大多数*服务器？
- en: Do *all* quorums have to intersect? In other words, does a quorum we use to
    pick a distinguished proposer (first phase), a quorum we use to decide on a value
    (second phase), and every execution instance (for example, if multiple instances
    of the second step are executed concurrently), have to have nodes in common?
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有法定人数都必须有交集吗？换句话说，我们用来选择特定提议者（第一阶段）、决定一个值（第二阶段）以及每个执行实例（例如，如果多个第二步实例同时执行），都必须有共同节点吗？
- en: 'Since we’re still talking about consensus, we cannot change any safety definitions:
    the algorithm has to guarantee the agreement.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仍在讨论共识，我们不能改变任何安全性定义：算法必须保证协议。
- en: In Multi-Paxos, the leader election phase is infrequent, and the distinguished
    proposer is allowed to commit several values without rerunning the election phase,
    potentially staying in the lead for a longer period. In [“Tunable Consistency”](ch11.html#tunable_consistency),
    we discussed formulae that help us to find configurations where we have intersections
    between the node sets. One of the examples was to wait for just one node to acknowledge
    the write (and let the requests to the rest of nodes finish asynchronously), and
    read from *all* the nodes. In other words, as long as we keep `R + W > N`, there’s
    at least one node in common between read and write sets.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Multi-Paxos 中，领导选举阶段不频繁，允许特定提议者提交多个值，而无需重新运行选举阶段，潜在地在领先位置停留更长时间。在[“可调一致性”](ch11.html#tunable_consistency)中，我们讨论了帮助我们找到节点集之间交集的公式。一个例子是等待仅一个节点确认写入（并让其余节点的请求异步完成），并从*所有*节点读取。换句话说，只要保持
    `R + W > N`，读写集之间至少有一个共同节点。
- en: Can we use a similar logic for consensus? It turns out that we can, and in Paxos
    we only require the group of nodes from the first phase (that elects a leader)
    to overlap with the group from the second phase (that participates in accepting
    proposals).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否对共识使用类似的逻辑？事实证明我们可以，在 Paxos 中，我们只需要第一阶段选举领导者的节点组与第二阶段参与接受提案的节点组有重叠。
- en: In other words, a quorum doesn’t have to be defined as a majority, but only
    as a non-empty group of nodes. If we define a total number of participants as
    `N`, the number of nodes required for a propose phase to succeed as `Q₁`, and
    the number of nodes required for the accept phase to succeed as `Q₂`, we only
    need to ensure that `Q₁ + Q₂ > N`. Since the second phase is usually more common
    than the first one, `Q₂` can contain only `N/2` acceptors, as long as `Q₁` is
    adjusted to be correspondingly larger (`Q₁ = N - Q₂ + 1`). This finding is an
    important observation crucial for understanding consensus. The algorithm that
    uses this approach is called *Flexible Paxos* [[HOWARD16]](app01.html#HOWARD16).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，法定人数不必定义为多数，而只需定义为非空节点组。如果我们将参与者总数定义为 `N`，提议阶段成功所需节点数为 `Q₁`，接受阶段成功所需节点数为
    `Q₂`，我们只需确保 `Q₁ + Q₂ > N`。由于第二阶段通常比第一阶段更常见，`Q₂` 可以仅包含 `N/2` 个接受者，只要相应地增加 `Q₁`（`Q₁
    = N - Q₂ + 1`）。这一发现是理解共识至关重要的重要观察。使用这种方法的算法称为*灵活 Paxos* [[HOWARD16]](app01.html#HOWARD16)。
- en: For example, if we have five acceptors, as long as we require collecting votes
    from four of them to win the election round, we can allow the leader to wait for
    responses from two nodes during the replication stage. Moreover, since there’s
    an overlap between *any* subset consisting of two acceptors with the leader election
    quorum, we can submit proposals to disjoint sets of acceptors. Intuitively, this
    works because whenever a new leader is elected without the current one being aware
    of it, there will always be at least one acceptor that knows about the existence
    of the new leader.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有五个接受者，只要我们要求从其中四个收集投票来赢得选举轮次，我们可以允许领导者在复制阶段等待两个节点的响应。此外，由于在包含领导者选举法定人数的任何子集中存在重叠，我们可以向不相交的接受者集合提交提案。直觉上，这有效是因为每当新领导者在当前领导者不知情的情况下被选举时，总会至少有一个接受者知道新领导者的存在。
- en: 'Flexible Paxos allows trading availability for latency: we reduce the number
    of nodes participating in the second phase but have to collect more votes, requiring
    more participants to be available during the leader election phase. The good news
    is that this configuration can continue the replication phase and tolerate failures
    of up to `N - Q₂` nodes, as long as the current leader is stable and a new election
    round is not required.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Flexible Paxos 允许在可用性和延迟之间进行权衡：我们减少第二阶段参与的节点数，但必须收集更多的投票，需要在领导者选举阶段期间有更多参与者可用。好消息是，只要当前领导者稳定且不需要新的选举轮次，这种配置可以继续复制阶段并容忍最多
    `N - Q₂` 节点的故障。
- en: Another Paxos variant using the idea of intersecting quorums is Vertical Paxos.
    Vertical Paxos distinguishes between read and write quorums. These quorums must
    intersect. A leader has to collect a smaller read quorum for one or more lower-numbered
    proposals, and a larger write quorum for its own proposal [[LAMPORT09]](app01.html#LAMPORT09).
    [[LAMPSON01]](app01.html#LAMPSON01) also distinguishes between the *out* and *decision*
    quorums, which translate to prepare and accept phases, and gives a quorum definition
    similar to Flexible Paxos.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使用交叉法定人数思想的 Paxos 变体是 Vertical Paxos。Vertical Paxos 区分读和写法定人数。这些法定人数必须相交。领导者必须为一个或多个较低编号的提案收集较小的读法定人数，并为自己的提案收集更大的写法定人数（[LAMPORT09](app01.html#LAMPORT09)）。[LAMPSON01](app01.html#LAMPSON01)也区分了*out*和*decision*法定人数，这对应于准备和接受阶段，并给出了类似于
    Flexible Paxos 的法定人数定义。
- en: Generalized Solution to Consensus
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般化的共识解决方案
- en: 'Paxos might sometimes be a bit difficult to reason about: multiple roles, steps,
    and all the possible variations are hard to keep track of. But we can think of
    it in simpler terms. Instead of splitting roles between the participants and having
    decision rounds, we can use a simple set of concepts and rules to achieve guarantees
    of a single-decree Paxos. We discuss this approach only briefly as this is a relatively
    new development [[HOWARD19]](app01.html#HOWARD19)—it’s important to know, but
    we’ve yet to see its implementations and practical applications.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有时理解 Paxos 可能有些困难：多个角色、步骤以及所有可能的变化很难跟踪。但我们可以用更简单的术语来思考它。不再将角色分配给参与者并进行决策轮次，而是使用一组简单的概念和规则来实现单指令
    Paxos 的保证。我们只简要讨论这种方法，因为这是一个相对新的发展（[HOWARD19](app01.html#HOWARD19)）—这很重要，但我们还没有看到它的实现和实际应用。
- en: 'We have a client and a set of servers. Each server has multiple *registers*.
    A register has an index identifying it, can be written only once, and it can be
    in one of three states: unwritten, containing a *value*, and containing *nil*
    (a special empty value).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个客户端和一组服务器。每个服务器都有多个*寄存器*。一个寄存器有一个标识它的索引，只能写入一次，并且可以处于三种状态之一：未写入、包含一个*值*和包含*nil*（特殊的空值）。
- en: 'Registers with the same index located on different servers form a *register
    set*. Each register set can have one or more quorums. Depending on the state of
    the registers in it, a quorum can be in one of the *undecided* (`Any` and `Maybe
    v`), or *decided* (`None` and `Decided v`) states:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 存储在不同服务器上的相同索引的寄存器形成一个*寄存器集*。每个寄存器集可以有一个或多个法定人数。根据其中寄存器的状态，法定人数可以处于*未决*（`Any`
    和 `Maybe v`）或*已决*（`None` 和 `Decided v`）状态之一：
- en: '`Any`'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`Any`'
- en: Depending on future operations, this quorum set can decide on any value.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 根据未来操作的不同，这个法定人数集合可以决定任何值。
- en: '`Maybe v`'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`Maybe v`'
- en: If this quorum reaches a decision, its decision can only be `v`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个法定人数达成决定，它的决定只能是 `v`。
- en: '`None`'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`None`'
- en: This quorum cannot decide on the value.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个法定人数无法决定值。
- en: '`Decided v`'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`Decided v`'
- en: This quorum has decided on the value `v`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个法定人数已经决定了值 `v`。
- en: The client exchanges messages with the servers and maintains a state table,
    where it keeps track of values and registers, and can infer decisions made by
    the quorums.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端与服务器进行消息交换并维护状态表，在其中跟踪数值和寄存器，并能推断出由法定人数做出的决策。
- en: To maintain correctness, we have to limit how clients can interact with servers
    and which values they may write and which they may not. In terms of reading values,
    the client can output the decided value only if it has read it from the quorum
    of servers in the same register set.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持正确性，我们必须限制客户端与服务器的交互方式及其可写入的值和不可写入的值。在读取值方面，只有当客户端从同一寄存器集的法定人数服务器中读取了决策值时，才能输出该值。
- en: 'The writing rules are slightly more involved because to guarantee algorithm
    safety, we have to preserve several invariants. First, we have to make sure that
    the client doesn’t just come up with new values: it is allowed to write a specific
    value to the register only if it has received it as input or has read it from
    a register. Clients cannot write values that allow different quorums in the same
    register to decide on different values. Lastly, clients cannot write values that
    override previous decisions made in the previous register sets (decisions made
    in register sets up to `r - 1` have to be `None`, `Maybe v`, or `Decided v`).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 写入规则稍微复杂，因为为了确保算法的安全性，我们必须保留几个不变量。首先，我们必须确保客户端不会随意提出新值：客户端只有在接收到输入或从寄存器中读取了该值后，才能将特定值写入寄存器。客户端不能写入允许在同一寄存器中不同法定人数做出不同值决策的值。最后，客户端不能覆盖先前在前一个寄存器集中做出的决策（在寄存器集直到`r
    - 1`的决策必须是`None`、`Maybe v`或`Decided v`）。
- en: Generalized Paxos algorithm
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泛化 Paxos 算法
- en: Putting all these rules together, we can implement a generalized Paxos algorithm
    that achieves consensus over a single value using write-once registers [[HOWARD19]](app01.html#HOWARD19).
    Let’s say we have three servers `[S0, S1, S2]`, registers `[R0, R1, …]`, and clients
    `[C0, C1, ...]`, where the client can only write to the assigned subset of registers.
    We use simple majority quorums for all registers (`{S0, S1}`, `{S0, S2}`, `{S1,
    S2}`).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 综合所有这些规则，我们可以实现一种通用的 Paxos 算法，通过一次写入寄存器来实现单一值的共识[[HOWARD19]](app01.html#HOWARD19)。假设我们有三台服务器`[S0,
    S1, S2]`，寄存器`[R0, R1, …]`和客户端`[C0, C1, ...]`，其中客户端只能写入分配的寄存器子集。我们对所有寄存器使用简单的法定人数法（`{S0,
    S1}`, `{S0, S2}`, `{S1, S2}`）。
- en: 'The decision process here consists of two phases. The first phase ensures that
    it is safe to write a value to the register, and the second phase writes the value
    to the register:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的决策过程包括两个阶段。第一阶段确保安全地将值写入寄存器，第二阶段将该值写入寄存器：
- en: During phase 1
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段
- en: The client checks if the register it is about to write is unwritten by sending
    a `P1[A](register)` command to the server. If the register is unwritten, all registers
    up to `register - 1` are set to `nil`, which prevents clients from writing to
    previous registers. The server responds with a set of registers written so far.
    If it receives responses from the majority of servers, the client chooses either
    the nonempty value from the register with the largest index or its own value in
    case no value is present. Otherwise, it restarts the first phase.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端通过向服务器发送`P1[A](register)`命令来检查它即将写入的寄存器是否未被写入。如果该寄存器未被写入，则将所有寄存器设置为`nil`，以防止客户端写入先前的寄存器。服务器会响应当前已写入的一组寄存器。如果收到大多数服务器的响应，客户端会选择寄存器中具有最大索引的非空值或自身的值（如果没有值）。否则，将重新开始第一阶段。
- en: During phase 2
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段
- en: The client notifies all servers about the value it has picked during the first
    phase by sending them `P2[A](register, value)`. If the majority of servers respond
    to this message, it can output the decision value. Otherwise, it starts again
    from phase 1.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端通过向所有服务器发送`P2[A](register, value)`来通知它们在第一阶段选择的值。如果大多数服务器对此消息做出响应，则可以输出决策值。否则，将从第一阶段重新开始。
- en: '[Figure 14-9](#generalized_consensus_1) shows this generalization of Paxos
    (adapted from [[HOWARD19]](app01.html#HOWARD19)). Client `C0` tries to commit
    value `V`. During the first step, its state table is empty, and servers `S0` and
    `S1` respond with the empty register set, indicating that no registers were written
    so far. During the second step, it can submit its value `V`, since no other value
    was written.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-9](#generalized_consensus_1) 显示了 Paxos 的这种泛化形式（改编自[[HOWARD19]](app01.html#HOWARD19)）。客户端`C0`尝试提交值`V`。在第一步中，它的状态表为空，并且服务器`S0`和`S1`响应空寄存器集，表示到目前为止未写入任何寄存器。在第二步中，它可以提交其值`V`，因为之前没有写入其他值。'
- en: '![dbin 1409](assets/dbin_1409.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1409](assets/dbin_1409.png)'
- en: Figure 14-9\. Generalization of Paxos
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-9\. Paxos的泛化
- en: At that point, any other client can query servers to find out the current state.
    Quorum `{S0, S1}` has reached `Decided A` state, and quorums `{S0, S2}` and `{S1,
    S2}` have reached the `Maybe V` state for `R0`, so `C1` chooses the value `V`.
    At that point, no client can decide on a value other than `V`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在那时，任何其他客户端都可以查询服务器以了解当前状态。`Quorum {S0, S1}`已达到`Decided A`状态，并且`Quorum {S0,
    S2}`和`{S1, S2}`已达到`Maybe V`状态以用于`R0`，因此`C1`选择值`V`。此时，没有客户端可以决定除`V`之外的其他值。
- en: This approach helps to understand the semantics of Paxos. Instead of thinking
    about the state from the perspective of interactions of remote actors (e.g., a
    proposer finding out whether or not an acceptor has already accepted a different
    proposal), we can think in terms of the last known state, making our decision
    process simple and removing possible ambiguities. Immutable state and message
    passing can also be easier to implement correctly.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有助于理解Paxos的语义。与其从远程参与者的互动角度思考状态（例如，提议者查明接受者是否已经接受了不同的提议），我们可以从最后已知的状态来思考，使我们的决策过程简单化并消除可能的歧义。不可变状态和消息传递也更容易正确实现。
- en: We can also draw parallels with original Paxos. For example, in a scenario in
    which the client finds that one of the previous register sets has the `Maybe V`
    decision, it picks up `V` and attempts to commit it again, which is similar to
    how a proposer in Paxos can propose the value after the failure of the previous
    proposer that was able to commit the value to at least one acceptor. Similarly,
    if in Paxos leader conflicts are resolved by restarting the vote with a higher
    proposal number, in the generalized algorithm any unwritten lower-ranked registers
    are set to `nil`.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以与原始Paxos进行类比。例如，在客户端发现以前的寄存器集合有`Maybe V`决策的情况下，它会选取`V`并尝试再次提交，这与Paxos中的提议者在之前能够向至少一个接受者提交值后失败后提出值的方式类似。同样地，如果在Paxos中通过以更高的提议号重新开始投票来解决领导者冲突，那么在这种广义算法中，任何未写入的低级寄存器都将被设置为`nil`。
- en: Raft
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Raft
- en: Paxos was *the* consensus algorithm for over a decade, but in the distributed
    systems community it’s been known as difficult to reason about. In 2013, a new
    algorithm called Raft appeared. The researchers who developed it wanted to create
    an algorithm that’s easy to understand and implement. It was first presented in
    a paper titled “In Search of an Understandable Consensus Algorithm” [[ONGARO14]](app01.html#ONGARO14).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Paxos曾经是十多年来的*共识算法*，但在分布式系统社区中，它被认为难以理解。2013年，一种名为Raft的新算法出现了。开发它的研究人员希望创建一种易于理解和实现的算法。它首次在题为“寻找易于理解的共识算法”的论文中被提出[[ONGARO14]](app01.html#ONGARO14)。
- en: There’s enough inherent complexity in distributed systems, and having simpler
    algorithms is very desirable. Along with a paper, the authors have released a
    reference implementation called [LogCabin](https://databass.dev/links/69) to resolve
    possible ambiguities and help future implementors to gain a better understanding.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中有足够的固有复杂性，因此拥有更简单的算法是非常可取的。除了一篇论文外，作者还发布了一个名为[LogCabin](https://databass.dev/links/69)的参考实现，以解决可能存在的歧义并帮助未来的实现者更好地理解。
- en: 'Locally, participants store a log containing the sequence of commands executed
    by the state machine. Since inputs that processes receive are identical and logs
    contain the same commands in the same order, applying these commands to the state
    machine guarantees the same output. Raft simplifies consensus by making the concept
    of leader a first-class citizen. A leader is used to coordinate state machine
    manipulation and replication. There are many similarities between Raft and atomic
    broadcast algorithms, as well as Multi-Paxos: a single leader emerges from replicas,
    makes atomic decisions, and establishes the message order.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地，参与者存储包含状态机执行命令序列的日志。由于进程接收的输入相同且日志以相同顺序包含相同命令，将这些命令应用于状态机可以保证相同的输出。Raft通过将领导者的概念提升为一等公民来简化共识。领导者用于协调状态机操作和复制。Raft与原子广播算法以及Multi-Paxos之间有许多相似之处：一个单一领导者从副本中出现，做出原子决策并确立消息顺序。
- en: 'Each participant in Raft can take one of three roles:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Raft中的每个参与者可以承担三种角色之一：
- en: Candidate
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 候选者
- en: Leadership is a temporary condition, and any participant can take this role.
    To become a leader, the node first has to transition into a candidate state, and
    attempt to collect a majority of votes. If a candidate neither wins nor loses
    the election (the vote is split between multiple candidates and none of them has
    a majority of votes), the new term is slated and election restarts.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 领导权是一种临时条件，任何参与者都可以担任此角色。为了成为领导者，节点首先必须转换为候选状态，并尝试收集大多数选票。如果候选者既没有赢得选举也没有失败（选票在多个候选者之间分布，没有人获得大多数选票），则安排新term并重新开始选举。
- en: Leader
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者
- en: A current, temporary cluster leader that handles client requests and interacts
    with a replicated state machine. The leader is elected for a period called a *term*.
    Each term is identified by a monotonically increasing number and may continue
    for an arbitrary time period. A new leader is elected if the current one crashes,
    becomes unresponsive, or is suspected by other processes to have failed, which
    can happen because of network partitions and message delays.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，临时的集群领导者处理客户端请求并与复制状态机交互。领导者被选举为称为*term*的时间段。每个term由一个单调增加的数字标识，可以持续任意时间。如果当前领导者崩溃、变得无响应或被其他进程怀疑失败（这可能是由于网络分区和消息延迟引起的），将选举新的领导者。
- en: Follower
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 追随者
- en: A passive participant that persists log entries and responds to requests from
    the leader and candidates. Follower in Raft is a role similar to acceptor *and*
    learner from Paxos. Every process begins as a follower.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 作为被动参与者，持久化日志条目并响应来自领导者和候选者的请求。Raft中的追随者角色类似于Paxos中的接受者*和*学习者。每个进程最初都是一个追随者。
- en: To guarantee global partial ordering without relying on clock synchronization,
    time is divided into *terms* (also called epoch), during which the leader is unique
    and stable. Terms are monotonically numbered, and each command is uniquely identified
    by the term number and the message number within the term [[HOWARD14]](app01.html#HOWARD14).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证全局部分排序而不依赖时钟同步，时间被分割为*terms*（也称为时代），在此期间领导者是唯一且稳定的。terms是单调编号的，并且每个命令由term编号和term内消息编号唯一标识
    [[HOWARD14]](app01.html#HOWARD14)。
- en: It may happen that different participants disagree on which term is *current*,
    since they can find out about the new term at different times, or could have missed
    the leader election for one or multiple terms. Since each message contains a term
    identifier, if one of the participants discovers that its term is out-of-date,
    it updates the term to the higher-numbered one [[ONGARO14]](app01.html#ONGARO14).
    This means that there *may be* several terms in flight at any given point in time,
    but the higher-numbered one wins in case of a conflict. A node updates the term
    only if it starts a new election process or finds out that its term is out-of-date.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现不同参与者对哪个term是*当前*存在分歧的情况，因为它们可能在不同的时间发现新term，或者可能错过了一个或多个term的领导者选举。由于每个消息包含一个term标识符，如果参与者之一发现其term已过时，则将term更新为更高编号的term
    [[ONGARO14]](app01.html#ONGARO14)。这意味着在任何给定时间可能会*存在*多个term，但在冲突时以更高编号的term为准。节点仅在启动新选举过程或发现其term已过时时才更新term。
- en: On startup, or whenever a follower doesn’t receive messages from the leader
    and suspects that it has crashed, it starts the leader election process. A participant
    attempts to become a leader by transitioning into the candidate state and collecting
    votes from the majority of nodes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动时，或者当追随者没有收到来自领导者的消息并怀疑它已经崩溃时，它会启动领导者选举过程。参与者尝试通过转换为候选状态并从大多数节点收集选票来成为领导者。
- en: '[Figure 14-10](#raft_consensus_2) shows a sequence diagram representing the
    main components of the Raft algorithm:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-10](#raft_consensus_2) 显示了表示Raft算法主要组件的序列图：'
- en: Leader election
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者选举
- en: Candidate `P1` sends a `RequestVote` message to the other processes. This message
    includes the candidate’s term, the last term known by it, and the ID of the last
    log entry it has observed. After collecting a majority of votes, the candidate
    is successfully elected as a leader for the term. Each process can give its vote
    to at most one candidate.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 候选者 `P1` 向其他进程发送 `RequestVote` 消息。此消息包括候选者的term、它所知的最后一个term以及它观察到的最后一条日志条目的ID。在收集到大多数选票后，候选者成功当选为该term的领导者。每个进程最多可以将其选票给一个候选者。
- en: Periodic heartbeats
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 定期心跳
- en: The protocol uses a heartbeat mechanism to ensure the liveness of participants.
    The leader periodically sends heartbeats to all followers to maintain its term.
    If a follower doesn’t receive new heartbeats for a period called an *election
    timeout*, it assumes that the leader has failed and starts a new election.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 协议使用心跳机制来确保参与者的活性。领导者定期向所有跟随者发送心跳以维护其任期。如果跟随者在称为 *选举超时* 的一段时间内未收到新的心跳，则假定领导者已失败，并开始新的选举。
- en: Log replication / broadcast
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 日志复制 / 广播
- en: The leader can repeatedly append new values to the replicated log by sending
    `AppendEntries` messages. The message includes the leader’s term, index, and term
    of the log entry that immediately precedes the ones it’s currently sending, and
    *one or more* entries to store.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者可以通过发送 `AppendEntries` 消息重复追加新值到复制日志中。该消息包括领导者的任期、索引，以及当前正在发送的日志条目之前的条目的任期和索引，以及
    *一个或多个* 要存储的条目。
- en: '![dbin 1410](assets/dbin_1410.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1410](assets/dbin_1410.png)'
- en: Figure 14-10\. Raft consensus algorithm summary
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-10\. Raft 一致性算法摘要
- en: Leader Role in Raft
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Raft 中的领导者角色
- en: 'A leader can be elected only from the nodes holding all committed entries:
    if during the election, the follower’s log information is more up-to-date (in
    other words, has a higher term ID, or a longer log entry sequence, if terms are
    equal) than the candidate’s, its vote is denied.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 领导者只能从持有所有已提交条目的节点中选举出来：如果在选举期间，跟随者的日志信息比候选者更为最新（换句话说，具有更高的任期 ID，或者如果任期相同，则具有更长的日志条目序列），则其投票被拒绝。
- en: To win the vote, a candidate has to collect a majority of votes. Entries are
    always replicated in order, so it is always enough to compare IDs of the latest
    entries to understand whether or not one of the participants is up-to-date.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为赢得选票，候选者必须收集到多数票。条目始终按顺序复制，因此只需比较最新条目的 ID 就足以了解参与者是否已经是最新状态。
- en: Once elected, the leader has to accept client requests (which can also be forwarded
    to it from other nodes) and replicate them to the followers. This is done by appending
    the entry to its log and sending it to all the followers in parallel.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦当选，领导者必须接受客户端请求（也可以从其他节点转发到它），并将它们复制到跟随者。这是通过将条目附加到其日志并并行发送到所有跟随者来完成的。
- en: When a follower receives an `AppendEntries` message, it appends the entries
    from the message to the local log, and acknowledges the message, letting the leader
    know that it was persisted. As soon as enough replicas send their acknowledgments,
    the entry is considered committed and is marked correspondingly in the leader
    log.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当跟随者收到 `AppendEntries` 消息时，它将消息中的条目追加到本地日志中，并确认该消息，告知领导者已持久化。一旦足够多的副本发送了确认，条目被视为已提交，并相应地在领导者日志中标记。
- en: Since only the most up-to-date candidates can become a leader, followers never
    have to bring the leader up-to-date, and log entries are only flowing from leader
    to follower and not vice versa.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 由于只有最新的候选者才能成为领导者，跟随者无需将领导者更新到最新状态，日志条目仅从领导者流向跟随者，而不是反过来。
- en: '[Figure 14-11](#raft_consensus_1) shows this process:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-11](#raft_consensus_1) 显示了这一过程：'
- en: a) A new command `x = 8` is appended to the leader’s log.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a) 新命令 `x = 8` 被追加到领导者日志中。
- en: b) Before the value can be committed, it has to be replicated to the majority
    of participants.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b) 在值能够提交之前，它必须复制到大多数参与者。
- en: c) As soon as the leader is done with replication, it commits the value locally.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: c) 一旦领导者完成复制，它就会在本地提交该值。
- en: d) The commit decision is replicated to the followers.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: d) 提交决策被复制到跟随者。
- en: '![dbin 1411](assets/dbin_1411.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1411](assets/dbin_1411.png)'
- en: Figure 14-11\. Procedure of a commit in Raft with `P[1]` as a leader
  id: totrans-246
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-11\. Raft 中以 `P[1]` 为领导者的提交过程
- en: '[Figure 14-12](#raft_consensus_3) shows an example of a consensus round where
    `P₁` is a leader, which has the most recent view of the events. The leader proceeds
    by replicating the entries to the followers, and committing them after collecting
    acknowledgments. Committing an entry also commits all entries preceding it in
    the log. Only the leader can make a decision on whether or not the entry can be
    committed. Each log entry is marked with a term ID (a number in the top-right
    corner of each log entry box) and a log index, identifying its position in the
    log. Committed entries are guaranteed to be replicated to the quorum of participants
    and are safe to be applied to the state machine in the order they appear in the
    log.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-12](#raft_consensus_3) 展示了一个共识轮的示例，其中 `P₁` 是领导者，具有最新的事件视图。领导者通过将条目复制到跟随者，并在收集确认后提交它们来继续。提交一个条目也会提交其在日志中的所有前置条目。只有领导者可以决定条目是否可以被提交。每个日志条目都标有术语ID（出现在每个日志条目框的右上角的数字）和日志索引，标识其在日志中的位置。已提交的条目保证已被复制到参与者的法定人数，并且可以安全地按照它们在日志中出现的顺序应用到状态机中。'
- en: '![dbin 1412](assets/dbin_1412.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1412](assets/dbin_1412.png)'
- en: Figure 14-12\. Raft state machine
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-12\. Raft 状态机
- en: Failure Scenarios
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 失败场景
- en: When multiple followers decide to become candidates, and no candidate can collect
    a majority of votes, the situation is called a *split vote*. Raft uses randomized
    timers to reduce the probability of multiple subsequent elections ending up in
    a split vote. One of the candidates can start the next election round earlier
    and collect enough votes, while the others sleep and give way to it. This approach
    speeds up the election without requiring any additional coordination between candidates.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个跟随者决定成为候选人，并且没有候选人能够收集到多数票时，这种情况称为*分裂投票*。Raft 使用随机定时器来减少多个连续选举最终导致分裂投票的概率。其中一个候选人可以更早地开始下一轮选举，并收集足够的选票，而其他人则进入休眠状态，让路给它。这种方法加速了选举过程，而无需候选人之间的额外协调。
- en: Followers may be down or slow to respond, and the leader has to make the best
    effort to ensure message delivery. It can try sending messages again if it doesn’t
    receive an acknowledgment within the expected time bounds. As a performance optimization,
    it can send multiple messages in parallel.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随者可能处于宕机或响应缓慢状态，领导者必须尽力确保消息的传递。如果在预期的时间范围内没有收到确认，则可以尝试重新发送消息。作为性能优化，可以并行发送多条消息。
- en: Since entries replicated by the leader are uniquely identified, repeated message
    delivery is guaranteed not to break the log order. Followers deduplicate messages
    using their sequence IDs, ensuring that double delivery has no undesired side
    effects.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于领导者复制的条目具有唯一标识，重复的消息传递确保不会破坏日志顺序。跟随者使用它们的序列ID对消息进行去重，确保重复传递不会产生不希望的副作用。
- en: Sequence IDs are also used to ensure the log ordering. A follower rejects a
    higher-numbered entry if the ID and term of the entry that immediately precedes
    it, sent by the leader, do not match the highest entry according to its own records.
    If entries in two logs on different replicas have the same term and the same index,
    they store the same command and all entries that precede them are the same.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 序列ID也用于确保日志的顺序。如果跟随者拒绝了领导者发送的ID和术语不匹配的编号更高的条目，则其自己记录中最高的条目。如果两个不同副本上的日志中的条目具有相同的术语和相同的索引，则它们存储相同的命令，并且其前置条目相同。
- en: Raft guarantees to never show an uncommitted message as a committed one, but,
    due to network or replica slowness, already committed messages can still be seen
    as *in progress*, which is a rather harmless property and can be worked around
    by retrying a client command until it is finally committed [[HOWARD14]](app01.html#HOWARD14).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Raft 保证永远不会将未提交的消息显示为已提交的消息，但是由于网络或副本速度慢的原因，已经提交的消息仍然可以看作是*正在进行中*的状态，这是一个相对无害的属性，可以通过重试客户端命令直到最终提交来解决
    [[HOWARD14]](app01.html#HOWARD14)。
- en: 'For failure detection, the leader has to send heartbeats to the followers.
    This way, the leader maintains its term. When one of the nodes notices that the
    current leader is down, it attempts to initiate the election. The newly elected
    leader has to restore the state of the cluster to the last known up-to-date log
    entry. It does so by finding a *common ground* (the highest log entry on which
    both the leader and follower agree), and ordering followers to *discard* all (uncommitted)
    entries appended after this point. It then sends the most recent entries from
    its log, overwriting the followers’ history. The leader’s own log records are
    never removed or overwritten: it can only append entries to its own log.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测故障，领导者必须向跟随者发送心跳。这样，领导者就能保持其任期。当其中一个节点注意到当前的领导者已经宕机时，它尝试启动选举。新选出的领导者必须将集群状态恢复到最后已知的最新日志条目。它通过找到一个*共同的基础*（领导者和跟随者都同意的最高日志条目）来做到这一点，并命令跟随者*丢弃*在此点之后追加的所有（未提交的）条目。然后，它发送其日志中的最新条目，覆盖跟随者的历史记录。领导者自己的日志记录永远不会被删除或覆盖：它只能向自己的日志追加条目。
- en: 'Summing up, the Raft algorithm provides the following guarantees:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，Raft 算法提供以下保证：
- en: Only one leader can be elected at a time for a given term; no two leaders can
    be active during the same term.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定任期内一次只能选出一个领导者；在同一任期内不能有两个领导者同时活跃。
- en: The leader does not remove or reorder its log contents; it only appends new
    messages to it.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领导者不会移除或重新排序其日志内容；它只会向其追加新的消息。
- en: Committed log entries are guaranteed to be present in logs for subsequent leaders
    and cannot get reverted, since before the entry is committed it is known to be
    replicated by the leader.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已提交的日志条目保证在后续领导者的日志中存在，并且不会被还原，因为在提交条目之前，领导者已知它已经被复制。
- en: All messages are identified uniquely by the message and term IDs; neither current
    nor subsequent leaders can reuse the same identifier for the different entry.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有消息都通过消息和任期 ID 唯一标识；当前和后续的领导者都不能为不同的条目重复使用相同的标识符。
- en: Since its appearance, Raft has become very popular and is currently used in
    many databases and other distributed systems, including [CockroachDB](https://databass.dev/links/70),
    [Etcd](https://databass.dev/links/71), and [Consul](https://databass.dev/links/72).
    This can be attributed to its simplicity, but also may mean that Raft lives up
    to the promise of being a reliable consensus algorithm.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 自其问世以来，Raft 已经变得非常流行，目前被许多数据库和其他分布式系统使用，包括[CockroachDB](https://databass.dev/links/70)、[Etcd](https://databass.dev/links/71)和[Consul](https://databass.dev/links/72)。这可以归因于其简单性，但也可能意味着Raft能够履行作为一种可靠的共识算法的承诺。
- en: Byzantine Consensus
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拜占庭共识
- en: All the consensus algorithms we have been discussing so far assume non-Byzantine
    failures (see [“Arbitrary Faults”](ch08.html#byzantine_faults)). In other words,
    nodes execute the algorithm in “good faith” and do not try to exploit it or forge
    the results.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有共识算法都假设非拜占庭故障（见[“任意故障”](ch08.html#byzantine_faults)）。换句话说，节点以“诚实守信”的方式执行算法，不试图利用或伪造结果。
- en: As we will see, this assumption allows achieving consensus with a smaller number
    of available participants and with fewer round-trips required for a commit. However,
    distributed systems are sometimes deployed in potentially adversarial environments,
    where the nodes are not controlled by the same entity, and we need algorithms
    that can ensure a system can function correctly even if some nodes behave erratically
    or even maliciously. Besides ill intentions, Byzantine failures can also be caused
    by bugs, misconfiguration, hardware issues, or data corruption.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将要看到的，这一假设允许在可用参与者更少、提交所需的往返次数更少的情况下达成共识。然而，分布式系统有时部署在潜在敌对的环境中，其中节点不由同一实体控制，我们需要能够确保系统即使某些节点表现异常或恶意行为也能正常运行的算法。除了恶意意图外，拜占庭故障还可能由错误、错误配置、硬件问题或数据损坏引起。
- en: Most Byzantine consensus algorithms require `N²` messages to complete an algorithm
    step, where `N` is the size of the quorum, since each node in the quorum has to
    communicate with each other. This is required to cross-validate each step against
    other nodes, since nodes cannot rely on each other or on the leader and have to
    verify other nodes’ behaviors by comparing returned results with the majority
    responses.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数拜占庭共识算法要求完成算法步骤需要`N²`条消息，其中`N`是法定人数的大小，因为法定人数中的每个节点必须与其他节点通信。这是为了通过将返回结果与多数响应进行比较，验证其他节点的行为，因为节点不能依赖于彼此或领导者。
- en: 'We’ll only discuss one Byzantine consensus algorithm here, Practical Byzantine
    Fault Tolerance (PBFT) [[CASTRO99]](app01.html#CASTRO99). PBFT assumes independent
    node failures (i.e., failures can be coordinated, but the entire system cannot
    be taken over at once, or at least with the same exploit method). The system makes
    weak synchrony assumptions, like how you would expect a network to behave normally:
    failures may occur, but they are not indefinite and are eventually recovered from.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们只讨论一个拜占庭容错算法，实用拜占庭容错（PBFT）[[CASTRO99]](app01.html#CASTRO99)。PBFT 假设节点故障是独立的（即，故障可能是协调的，但整个系统不能同时被攻陷，或者至少不用同一种攻击方法）。系统对网络的同步性做出了弱假设，就像你期望网络正常运行一样：可能会发生故障，但不是无限期的，并且最终可以恢复。
- en: All communication between the nodes is encrypted, which serves to prevent message
    forging and network attacks. Replicas know one another’s public keys to verify
    identities and encrypt messages. Faulty nodes may leak information from inside
    the system, since, even though encryption is used, every node needs to interpret
    message contents to react upon them. This doesn’t undermine the algorithm, since
    it serves a different purpose.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 节点之间的所有通信都是加密的，这有助于防止消息伪造和网络攻击。副本知道彼此的公钥以验证身份并加密消息。由于即使使用了加密，故障节点可能会泄漏系统内部的信息，因为每个节点都需要解释消息内容以做出反应。这并不会损害算法的完整性，因为它服务于不同的目的。
- en: PBFT Algorithm
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PBFT 算法
- en: 'For PBFT to guarantee both safety and liveness, no more than `(n - 1)/3` replicas
    can be faulty (where `n` is the total number of participants). For a system to
    sustain `f` compromised nodes, it is required to have at least `n = 3f + 1` nodes.
    This is the case because a majority of nodes have to agree on the value: `f` replicas
    might be faulty, and there might be `f` replicas that are not responding but may
    not be faulty (for example, due to a network partition, power failure, or maintenance).
    The algorithm has to be able to collect enough responses from nonfaulty replicas
    to *still* outnumber those from the faulty ones.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证 PBFT 同时具有安全性和活性，最多可以有`(n - 1)/3`个副本是有故障的（其中 `n` 是参与者的总数）。对于一个系统要能够容忍 `f`
    个受损节点，至少需要有 `n = 3f + 1` 个节点。这是因为大多数节点必须就值达成一致：`f` 个副本可能是有故障的，而且可能有 `f` 个副本没有响应但可能不是有故障的（例如，由于网络分区、断电或维护）。算法必须能够从非有故障的副本收集足够的响应，以便*仍然*超过有故障副本的响应。
- en: 'Consensus properties for PBFT are similar to those of other consensus algorithms:
    all nonfaulty replicas have to agree both on the set of received values and their
    order, despite the possible failures.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: PBFT 的共识属性类似于其他共识算法：所有非有故障的副本必须就接收到的值的集合及其顺序达成一致，尽管可能存在故障。
- en: To distinguish between cluster configurations, PBFT uses *views*. In each view,
    one of the replicas is a *primary* and the rest of them are considered *backups*.
    All nodes are numbered consecutively, and the index of the primary node is `v
    mod N`, where `v` is the view ID, and `N` is the number of nodes in the current
    configuration. The view can change in cases when the primary fails. Clients execute
    their operations against the primary. The primary broadcasts the requests to the
    backups, which execute the requests and send a response back to the client. The
    client waits for `f + 1` replicas to respond with *the same result* for any operation
    to succeed.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分集群配置，PBFT 使用*视图*。在每个视图中，一个副本是*主节点*，其余的被视为*备份节点*。所有节点按顺序编号，主节点的索引为`v mod
    N`，其中 `v` 是视图 ID，`N` 是当前配置中节点的数量。当主节点失败时，视图可以改变。客户端对主节点执行其操作。主节点将请求广播给备份节点，备份节点执行请求并将响应发送给客户端。客户端等待`f
    + 1`个副本给出*相同的结果*以便操作成功。
- en: 'After the primary receives a client request, protocol execution proceeds in
    three phases:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点收到客户端请求后，协议执行分为三个阶段：
- en: Pre-prepare
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 预准备
- en: 'The primary broadcasts a message containing a view ID, a unique monotonically
    increasing identifier, a payload (client request), and a payload digest. Digests
    are computed using a strong collision-resistant hash function, and are signed
    by the sender. The backup accepts the message if its view matches with the primary
    view and the client request hasn’t been tampered with: the calculated payload
    digest matches the received one.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点广播包含视图 ID、唯一单调递增标识符、有效负载（客户端请求）和有效负载摘要的消息。摘要使用强碰撞抗性哈希函数计算，并由发送方签名。如果备份节点的视图与主视图匹配且客户端请求未被篡改，则备份节点接受该消息：计算得到的有效负载摘要与接收到的摘要匹配。
- en: Prepare
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 准备
- en: 'If the backup accepts the pre-prepare message, it enters the prepare phase
    and starts broadcasting `Prepare` messages, containing a view ID, message ID,
    and a payload digest, but without the payload itself, to all other replicas (including
    the primary). Replicas can move past the prepare state only if they receive `2f`
    prepares from *different* backups that match the message received during pre-prepare:
    they have to have the same view, same ID, and a digest.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如果备份接受预准备消息，则进入准备阶段并开始广播`Prepare`消息，包含视图 ID、消息 ID 和负载摘要，但不包括负载本身，发送至所有其他副本（包括主节点）。副本只有在收到来自*不同*备份的`2f`个匹配的准备消息，并且这些消息与预准备阶段接收到的消息相匹配时，才能继续进入准备状态：它们必须具有相同的视图、相同的
    ID 和摘要。
- en: Commit
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 提交
- en: After that, the backup moves to the commit phase, where it broadcasts `Commit`
    messages to all other replicas and waits to collect `2f + 1` matching `Commit`
    messages (possibly including its own) from the other participants.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，备份进入提交阶段，在这里向所有其他副本广播`Commit`消息，并等待收集来自其他参与者的`2f + 1`个匹配的`Commit`消息（可能包括自己的消息）。
- en: 'A *digest* in this case is used to reduce the message size during the prepare
    phase, since it’s not necessary to rebroadcast an entire payload for verification,
    as the digest serves as a payload summary. Cryptographic hash functions are resistant
    to collisions: it is difficult to produce two values that have the same digest,
    let alone two messages with matching digests that *make sense* in the context
    of the system. In addition, digests are *signed* to make sure that the digest
    itself is coming from a trusted source.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*摘要*用于减小准备阶段的消息大小，因为没有必要重新广播整个负载以进行验证，摘要充当负载摘要。密码哈希函数能够抵抗碰撞：很难产生两个具有相同摘要的值，更不用说在系统上下文中具有*有意义*的两个匹配摘要的消息了。此外，摘要是*签名*的，以确保摘要本身来自可信源。
- en: The number `2f` is important, since the algorithm has to make sure that at least
    `f + 1` nonfaulty replicas respond to the client.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 数字`2f`很重要，因为算法必须确保至少`f + 1`个非故障副本响应客户端。
- en: '[Figure 14-13](#pbft_consensus_1) shows a sequence diagram of a normal-case
    PBFT algorithm round: the client sends a request to `P[1]`, and nodes move between
    phases by collecting a sufficient number of matching responses from *properly
    behaving* peers. `P[4]` may have failed or could’ve responded with unmatching
    messages, so its responses wouldn’t have been counted.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14-13](#pbft_consensus_1)展示了正常情况下PBFT算法轮次的序列图：客户端向`P[1]`发送请求，节点通过从*行为正确*的对等方收集足够数量的匹配响应，在各个阶段之间移动。`P[4]`可能已失败或者响应的消息不匹配，因此其响应未被计入。'
- en: '![dbin 1413](assets/dbin_1413.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![dbin 1413](assets/dbin_1413.png)'
- en: Figure 14-13\. PBFT consensus, normal-case operation
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 14-13\. PBFT 共识，正常操作
- en: During the prepare and commit phases, nodes communicate by sending messages
    to each other node and waiting for the messages from the corresponding number
    of other nodes, to check if they match and make sure that incorrect messages are
    not broadcasted. Peers cross-validate all messages so that only nonfaulty nodes
    can successfully commit messages. If a sufficient number of matching messages
    cannot be collected, the node doesn’t move to the next step.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备和提交阶段，节点通过向每个节点发送消息并等待相应数量的消息来进行通信，以检查它们是否匹配，并确保不广播错误消息。同行交叉验证所有消息，以确保只有非故障节点能成功提交消息。如果无法收集到足够数量的匹配消息，则节点不会进入下一步。
- en: When replicas collect enough commit messages, they notify the client, finishing
    the round. The client cannot be certain about whether or not execution was fulfilled
    correctly until it receives `f + 1` matching responses.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当副本收集到足够的提交消息时，它们通知客户端，完成本轮。客户端只有在收到`f + 1`个匹配响应后才能确信执行是否正确。
- en: View changes occur when replicas notice that the primary is inactive, and suspect
    that it might have failed. Nodes that detect a primary failure stop responding
    to further messages (apart from checkpoint and view-change related ones), broadcast
    a view change notification, and wait for confirmations. When the primary of the
    new view receives `2f` view change events, it initiates a new view.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 视图变更发生在副本注意到主节点不活跃并怀疑其可能已失败时。检测到主节点失败的节点停止响应进一步的消息（除了检查点和视图变更相关的消息），广播视图变更通知，并等待确认。当新视图的主节点收到`2f`个视图变更事件时，它会启动新视图。
- en: To reduce the number of messages in the protocol, clients can collect `2f +
    1` matching responses from nodes that *tentatively* execute a request (e.g., after
    they’ve collected a sufficient number of matching `Prepared` messages). If the
    client cannot collect enough matching tentative responses, it retries and waits
    for `f + 1` nontentative responses as described previously.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少协议中的消息数量，客户端可以从执行请求的节点（例如在收集到足够数量的匹配的“准备”消息后）收集到`2f + 1`个匹配的响应。如果客户端无法收集到足够的匹配临时响应，则会重试，并等待如前所述的`f
    + 1`个非临时响应。
- en: Read-only operations in PBFT can be done in just one round-trip. The client
    sends a read request to all replicas. Replicas execute the request in their tentative
    states, after all ongoing state changes to the read value are committed, and respond
    to the client. After collecting `2f + 1` responses with the same value from different
    replicas, the operation completes.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在PBFT中，只读操作可以在一个往返中完成。客户端向所有副本发送读请求。副本在它们的临时状态下执行请求，待所有正在进行的状态更改对读值的提交完成后，向客户端响应。在从不同副本收集到相同值的`2f
    + 1`个响应之后，操作完成。
- en: Recovery and Checkpointing
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恢复与检查点
- en: Replicas save accepted messages in a stable log. Every message has to be kept
    until it has been executed by at least `f + 1` nodes. This log can be used to
    get other replicas up to speed in case of a network partition, but recovering
    replicas need some means of verifying that the state they receive is correct,
    since otherwise recovery can be used as an attack vector.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 副本将已接受的消息保存在稳定日志中。每条消息必须保留，直到至少`f + 1`个节点执行完毕。这个日志可以在网络分区的情况下帮助其他副本追赶进度，但是恢复的副本需要某种方法来验证它们接收到的状态是否正确，否则恢复可能会被用作攻击向量。
- en: To show that the state is correct, nodes compute a digest of the state for messages
    up to a given sequence number. Nodes can compare digests, verify state integrity,
    and make sure that messages they received during recovery add up to a correct
    final state. This process is too expensive to perform on every request.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明状态的正确性，节点计算直至给定序列号的消息状态摘要。节点可以比较摘要，验证状态完整性，并确保他们在恢复过程中接收到的消息累积到正确的最终状态。这个过程在每个请求上执行起来过于昂贵。
- en: After every `N` requests, where `N` is a configurable constant, the primary
    makes a *stable checkpoint*, where it broadcasts the latest sequence number of
    the latest request whose execution is reflected in the state, and the digest of
    this state. It then waits for `2f + 1` replicas to respond. These responses constitute
    a proof for this checkpoint, and a guarantee that replicas can safely discard
    state for all pre-prepare, prepare, commit, and checkpoint messages up to the
    given sequence number.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 每经过`N`个请求后，其中`N`是一个可配置的常数，主节点会做一个*稳定检查点*，在此检查点中广播最新请求的最新序列号，以及此状态的摘要。然后等待`2f
    + 1`个副本的响应。这些响应构成了该检查点的证明，并保证副本可以安全地丢弃所有与给定序列号之前的预准备、准备、提交和检查点消息相关的状态。
- en: Byzantine fault tolerance is essential to understand and is used in storage
    systems deployed in potentially adversarial networks. Most of the time, it is
    enough to authenticate and encrypt internode communication, but when there’s no
    trust between the parts of the system, algorithms similar to PBFT have to be employed.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 拜占庭容错是理解的关键，并且用于部署在潜在对抗网络中的存储系统中。大多数情况下，对节点间通信进行身份验证和加密就足够了，但当系统各部分之间缺乏信任时，必须采用类似于PBFT的算法。
- en: Since algorithms resistant to Byzantine faults impose significant overhead in
    terms of the number of exchanged messages, it is important to understand their
    use cases. Other protocols, such as the ones described in [[BAUDET19]](app01.html#BAUDET19)
    and [[BUCHMAN18]](app01.html#BUCHMAN18), attempt to optimize the PBFT algorithm
    for systems with a large number of participants.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 由于抵抗拜占庭故障的算法在交换消息数量方面带来显著的开销，因此理解它们的用例至关重要。其他协议，例如《BAUDET19》和《BUCHMAN18》中描述的协议，试图优化适用于参与者数量庞大系统的PBFT算法。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Consensus algorithms are one of the most interesting yet most complex subjects
    in distributed systems. Over the last few years, new algorithms and many implementations
    of the existing algorithms have emerged, which proves the rising importance and
    popularity of the subject.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 共识算法是分布式系统中最有趣但也最复杂的主题之一。在过去几年里，新的算法以及现有算法的许多实现已经出现，这证明了这一主题的日益重要和受欢迎程度。
- en: 'In this chapter, we discussed the classic Paxos algorithm, and several variants
    of Paxos, each one improving its different properties:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了经典的Paxos算法，以及几种改进其不同属性的Paxos变体：
- en: Multi-Paxos
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: Multi-Paxos
- en: Allows a proposer to retain its role and replicate multiple values instead of
    just one.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 允许提议者保留其角色并复制多个值，而不仅仅是一个。
- en: Fast Paxos
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Fast Paxos
- en: Allows us to reduce a number of messages by using *fast* rounds, when acceptors
    can proceed with messages from proposers other than the established leader.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*快速*轮次来减少消息数量，允许接受者处理来自非建立领导者的提议者的消息。
- en: EPaxos
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: EPaxos
- en: Establishes event order by resolving dependencies between submitted messages.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决提交消息之间的依赖关系来建立事件顺序。
- en: Flexible Paxos
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Flexible Paxos
- en: Relaxes quorum requirements and only requires a quorum for the first phase (voting)
    to intersect with a quorum for the second phase (replication).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 放宽法定人数要求，并且只要求第一阶段（投票）的法定人数与第二阶段（复制）的法定人数交集。
- en: Raft simplifies the terms in which consensus is described, and makes leadership
    a first-class citizen in the algorithm. Raft separates log replication, leader
    election, and safety.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Raft简化了描述一致性的术语，并使领导力在算法中成为一等公民。Raft将日志复制、领导者选举和安全性分离。
- en: To guarantee consensus safety in adversarial environments, Byzantine fault-tolerant
    algorithms should be used; for example, PBFT. In PBFT, participants cross-validate
    one another’s responses and only proceed with execution steps when there’s enough
    nodes that obey the prescribed algorithm rules.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在对抗环境中保证共识安全性，应使用拜占庭容错算法；例如，PBFT。在PBFT中，参与者相互验证彼此的响应，并且只有在足够数量的节点遵守规定的算法规则时才进行执行步骤。
- en: ^([1](ch14.html#idm46466885073000-marker)) For example, such a situation was
    described in [*https://databass.dev/links/68*](https://databass.dev/links/68).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch14.html#idm46466885073000-marker)) 例如，在[*https://databass.dev/links/68*](https://databass.dev/links/68)中描述了这样的情况。
