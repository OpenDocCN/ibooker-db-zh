- en: Chapter 11\. Components of a Replica Set
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章\. 复制集的组件
- en: 'This chapter covers how the pieces of a replica set fit together, including:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了复制集中各个部件的组成，包括：
- en: How replica set members replicate new data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制集成员如何复制新数据
- en: How bringing up new members works
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何启动新成员的工作原理
- en: How elections work
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选举的工作原理
- en: Possible server and network failure scenarios
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能的服务器和网络故障场景
- en: Syncing
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同步
- en: Replication is concerned with keeping an identical copy of data on multiple
    servers. The way MongoDB accomplishes this is by keeping a log of operations,
    or *oplog*, containing every write that a primary performs. This is a capped collection
    that lives in the *local* database on the primary. The secondaries query this
    collection for operations to replicate.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 复制关注于在多个服务器上保持数据的相同副本。MongoDB 实现这一目标的方式是通过保持操作日志（oplog）的操作日志的记录，其中包含主服务器执行的每个写操作。这是一个固定大小的集合，存在于主服务器上的本地数据库中。从服务器查询此集合以获取要复制的操作。
- en: Each secondary maintains its own oplog, recording each operation it replicates
    from the primary. This allows any member to be used as a sync source for any other
    member, as shown in [Figure 11-1](#sync-0). Secondaries fetch operations from
    the member they are syncing from, apply the operations to their dataset, and then
    write the operations to their oplog. If applying an operation fails (which should
    only happen if the underlying data has been corrupted or in some way differs from
    the primary’s), the secondary will exit.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每个从服务器维护自己的操作日志，记录从主服务器复制的每个操作。这使得任何成员都可以作为任何其他成员的同步源，如 [图 11-1](#sync-0) 所示。从服务器从它们同步的成员获取操作，将这些操作应用于其数据集，然后将这些操作写入它们的操作日志。如果应用操作失败（这只会在底层数据已损坏或以某种方式与主服务器不同的情况下发生），从服务器将退出。
- en: '![](Images/mdb3_1101.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_1101.png)'
- en: Figure 11-1\. Oplogs keep an ordered list of write operations that have occurred;
    each member has its own copy of the oplog, which should be identical to the primary’s
    (modulo some lag)
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-1\. 操作日志（oplog）保持已发生写操作的有序列表；每个成员都有其自己的操作日志副本，应该与主服务器的相同（除了一些滞后）。
- en: 'If a secondary goes down for any reason, when it restarts it will start syncing
    from the last operation in its oplog. As operations are applied to data and then
    written to the oplog, the secondary may replay operations that it has already
    applied to its data. MongoDB is designed to handle this correctly: replaying oplog
    ops multiple times yields the same result as replaying them once. Each operation
    in the oplog is idempotent. That is, oplog operations produce the same results
    whether applied once or multiple times to the target dataset.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何原因导致辅助服务器宕机，在重新启动时，它将从其操作日志（oplog）中的最后一个操作开始同步。随着操作被应用到数据并写入操作日志，辅助服务器可能会重放已经应用到其数据的操作。MongoDB
    设计上能够正确处理这种情况：多次重放操作日志操作会产生与单次重放相同的结果。操作日志中的每个操作都是幂等的。也就是说，操作日志操作无论应用一次还是多次到目标数据集，都会产生相同的结果。
- en: 'Because the oplog is a fixed size, it can only hold a certain number of operations.
    In general, the oplog will use space at approximately the same rate as writes
    come into the system: if you’re writing 1 KB/minute on the primary, your oplog
    is probably going to fill up at about 1 KB/minute. However, there are a few exceptions:
    operations that affect multiple documents, such as removes or a multi-updates,
    will be exploded into many oplog entries. The single operation on the primary
    will be split into one oplog op per document affected. Thus, if you remove 1,000,000
    documents from a collection with `db.coll.remove()`, it will become 1,000,000
    oplog entries removing one document at a time. If you are doing lots of bulk operations,
    this can fill up your oplog more quickly than you might expect.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为操作日志（oplog）是固定大小的，它只能容纳一定数量的操作。一般来说，操作日志的使用空间速率大致与系统中写入的速率相同：如果在主服务器上每分钟写入
    1 KB，你的操作日志可能以每分钟约 1 KB 的速度填满。然而，也有几个例外情况：影响多个文档的操作，如删除或多文档更新，将会被分解为多个操作日志条目。主服务器上的单个操作将被拆分为每个受影响文档一个操作日志条目。因此，如果你使用
    `db.coll.remove()` 从集合中删除 1,000,000 个文档，它将变成 1,000,000 个逐个删除文档的操作日志条目。如果你执行大量的批量操作，这可能比你预期的更快地填满你的操作日志。
- en: 'In most cases, the default oplog size is sufficient. If you can predict your
    replica set’s workload to resemble one of the following patterns, then you might
    want to create an oplog that is larger than the default. Conversely, if your application
    predominantly performs reads with a minimal amount of write operations, a smaller
    oplog may be sufficient. These are the kinds of workloads that might require a
    larger oplog size:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，默认的操作日志大小已经足够。如果您能预测您的副本集工作负载类似以下哪种模式，那么您可能需要创建一个大于默认大小的操作日志。相反，如果您的应用程序主要执行读取操作，并且写入操作很少，则较小的操作日志可能足够。这些是可能需要较大操作日志大小的工作负载类型：
- en: Updates to multiple documents at once
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 同时更新多个文档
- en: The oplog must translate multi-updates into individual operations in order to
    maintain idempotency. This can use a great deal of oplog space without a corresponding
    increase in data size or disk use.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 操作日志必须将多个更新转换为单个操作，以保持幂等性。这可能会在不增加数据大小或磁盘使用的情况下使用大量操作日志空间。
- en: Deletions equal the same amount of data as inserts
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 删除和插入的数据量相等。
- en: If you delete roughly the same amount of data as you insert, the database will
    not grow significantly in terms of disk use, but the size of the operation log
    can be quite large.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您删除的数据量与插入的数据量大致相等，则数据库在磁盘使用方面不会显著增长，但操作日志的大小可能会非常大。
- en: Significant number of in-place updates
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的原地更新
- en: If a significant portion of the workload is updates that do not increase the
    size of the documents, the database records a large number of operations but the
    quantity of data on disk does not change.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果工作负载的一个重要部分是更新操作，但文档大小不增加，数据库记录的操作数量很多，但磁盘上的数据量不会改变。
- en: Before *mongod* creates an oplog, you can specify its size with the `oplogSizeMB`
    option. However, after you have started a replica set member for the first time,
    you can only change the size of the oplog using the [“Change the Size of the Oplog”
    procedure](https://oreil.ly/mh5SX).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在*mongod*创建操作日志之前，您可以使用`oplogSizeMB`选项指定其大小。但是，在首次启动副本集成员之后，只能使用[“更改操作日志大小”过程](https://oreil.ly/mh5SX)更改操作日志的大小。
- en: 'MongoDB uses two forms of data synchronization: an initial sync to populate
    new members with the full dataset, and replication to apply ongoing changes to
    the entire dataset. Let’s take a closer look at each of these.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB使用两种形式的数据同步：初始同步用于将新成员填充完整数据集，复制用于将持续变化应用于整个数据集。让我们更详细地看看每个过程。
- en: Initial Sync
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始同步
- en: MongoDB performs an initial sync to copy all the data from one member of the
    replica set to another member. When a member of the set starts up, it will check
    if it is in a valid state to begin syncing from someone. If it is in a valid state,
    it will attempt to make a full copy of the data from another member of the set.
    There are several steps to the process, which you can follow in the *mongod*’s
    log.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB执行初始同步，将所有数据从复制集的一个成员复制到另一个成员。当集合的成员启动时，它会检查是否处于可以开始从其他人同步的有效状态。如果处于有效状态，则会尝试从集合的另一个成员复制数据的完整副本。此过程包括几个步骤，您可以在*mongod*的日志中跟踪这些步骤。
- en: First, MongoDB clones all databases except the *local* database. The *mongod*
    scans every collection in each source database and inserts all the data into its
    own copies of these collections on the target member. Prior to beginning the clone
    operations, any existing data on the target member will be dropped.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，MongoDB克隆除*local*数据库之外的所有数据库。 *mongod*会扫描每个源数据库中的每个集合，并将所有数据插入到目标成员的这些集合的副本中。在开始克隆操作之前，目标成员上的任何现有数据都将被删除。
- en: Warning
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Only do an initial sync for a member if you do not want the data in your data
    directory or have moved it elsewhere, as *mongod*’s first action is to delete
    it all.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想在数据目录中存储数据或已将其移动到其他位置，只需为成员进行初始同步，因为*mongod*的第一步操作是将其全部删除。
- en: In MongoDB 3.4 and later, the initial sync builds all the collection indexes
    as the documents are copied for each collection (in earlier versions, only the
    `"_id"` indexes are built during this stage). It also pulls newly added oplog
    records during the data copy, so you should ensure that the target member has
    enough disk space in the *local* database to store these records during this data
    copy stage.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在MongoDB 3.4及更高版本中，初始同步会在为每个集合复制文档时构建所有集合索引（在早期版本中，仅在此阶段构建`"_id"`索引）。在数据复制期间还会拉取新增的操作日志记录，因此您应确保目标成员在此数据复制阶段具有足够的磁盘空间来存储*local*数据库中的这些记录。
- en: Once all the databases are cloned, the *mongod* uses the oplog from the source
    to update its dataset to reflect the current state of the replica set, applying
    all changes to the dataset that occurred while the copy was in progress. These
    changes might include any type of write (inserts, updates, and deletes), and this
    process might mean that *mongod* has to reclone certain documents that were moved
    and therefore missed by the cloner.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有数据库被克隆，*mongod*使用源的操作日志更新其数据集，以反映副本集的当前状态，将在复制过程中发生的所有更改应用于数据集。这些更改可能包括任何类型的写入（插入、更新和删除），这个过程可能意味着*mongod*必须重新克隆某些因为移动而被克隆器错过的文档。
- en: 'This is roughly what the logs will look like if some documents had to be recloned.
    Depending on the level of traffic and the types of operations that where happening
    on the sync source, you may or may not have missing objects:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某些文档必须重新克隆，则日志大致如下所示。根据同步源的流量级别和正在进行的操作类型，可能会有遗漏的对象或者不会有遗漏的对象。
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At this point, the data should exactly match the dataset as it existed at some
    point on the primary. The member finishes the initial sync process and transitions
    to normal syncing, which allows it to become a secondary.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，数据应该与主服务器在某个时间点存在的数据集完全匹配。成员完成初始同步过程并转换为正常同步，允许其成为次要成员。
- en: 'Doing an initial sync is very easy from an operator’s perspective: just start
    up a *mongod* with a clean data directory. However, it is often preferable to
    restore from a backup instead, as covered in [Chapter 23](ch23.xhtml#chapter-backup).
    Restoring from a backup is often faster than copying all of your data through
    *mongod*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从操作员的角度来看，执行初始同步非常简单：只需启动一个干净的数据目录下的*mongod*。然而，通常更倾向于从备份中恢复，如第23章所述。从备份中恢复通常比通过*mongod*复制所有数据快。
- en: Also, cloning can ruin the sync source’s working set. Many deployments end up
    with a subset of their data that’s frequently accessed and always in memory (because
    the OS is accessing it often). Performing an initial sync forces the member to
    page all of its data into memory, evicting the frequently used data. This can
    slow down a member dramatically as requests that were being handled by data in
    RAM are suddenly forced to go to disk. However, for small datasets and servers
    with some breathing room, initial syncing is a good, easy option.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，克隆可能会破坏同步源的工作集。许多部署最终会得到一个数据的子集，经常访问并始终在内存中（因为操作系统经常访问它）。执行初始同步会强制成员将其所有数据分页到内存中，驱逐经常使用的数据。这可能会显著减慢成员的速度，因为原本在RAM中处理的请求突然被迫访问磁盘。然而，对于小数据集和具有一定空间的服务器，初始同步是一个好的、简单的选择。
- en: 'One of the most common issues people run into with initial sync is it taking
    too long. In these cases, the new member can “fall off” the end of sync source’s
    oplog: it gets so far behind the sync source that it can no longer catch up because
    the sync source’s oplog has overwritten the data the member would need to use
    to continue replicating.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 初次同步过程中，人们经常遇到的一个最常见问题是花费的时间过长。在这些情况下，新成员可能会“掉队”，无法跟上同步源的操作日志的末尾：它落后于同步源，因为同步源的操作日志已经覆盖了成员需要使用的数据，无法继续复制。
- en: There is no way to fix this other than attempting the initial sync at a less
    busy time or restoring from a backup. The initial sync cannot proceed if the member
    has fallen off of the sync source’s oplog. [“Handling Staleness”](#handlingStaleness)
    covers this in more depth.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在较不忙的时候尝试初始同步或从备份中恢复外，没有其他修复此问题的方法。如果成员已经掉队同步源的操作日志，初始同步将无法继续。[“处理过时数据”](#handlingStaleness)将更深入地讨论此问题。
- en: Replication
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复制
- en: The second type of synchronization MongoDB performs is replication. Secondary
    members replicate data continuously after the initial sync. They copy the oplog
    from their sync source and apply these operations in an asynchronous process.
    Secondaries may automatically change their sync-from source as needed, in response
    to changes in the ping time and the state of other members’ replication. There
    are several rules that govern which members a given node can sync from. For example,
    replica set members with one vote cannot sync from members with zero votes, and
    secondaries avoid syncing from delayed members and hidden members. Elections and
    different classes of replica set members are discussed in later sections.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB执行的第二种同步类型是复制。从节点在初始同步后持续复制数据。它们从其同步源复制操作日志，并在异步过程中应用这些操作。从节点可能会根据需要自动更改它们的同步源，以响应
    ping 时间和其他成员复制状态的变化。有几条规则决定了给定节点可以从哪些成员进行同步。例如，具有一个投票的复制集成员不能从零投票的成员进行同步，而从节点避免从延迟成员和隐藏成员进行同步。选举和不同类别的复制集成员在后续部分中讨论。
- en: Handling Staleness
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理过时状态
- en: 'If a secondary falls too far behind the actual operations being performed on
    the sync source, the secondary will go *stale*. A stale secondary is unable to
    catch up because every operation in the sync source’s oplog is too far ahead:
    it would be skipping operations if it continued to sync. This could happen if
    the secondary has had downtime, has more writes than it can handle, or is too
    busy handling reads.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个从节点在同步源执行的实际操作中落后太多，它将会变得*过时*。一个过时的从节点无法追赶上来，因为同步源的操作日志中每个操作都太过超前：如果继续同步，它将会跳过操作。这可能发生在从节点停机期间、写入操作过多超出其处理能力，或者处理读取请求过于繁忙。
- en: When a secondary goes stale, it will attempt to replicate from each member of
    the set in turn to see if there’s anyone with a longer oplog that it can bootstrap
    from. If there is no one with a long-enough oplog, replication on that member
    will halt and it will need to be fully resynced (or restored from a more recent
    backup).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当从节点过时时，它将依次尝试从集合中的每个成员复制数据，以查看是否有更长操作日志可以引导它进行引导式同步。如果没有任何一个成员有足够长的操作日志，该成员的复制将停止，并且需要完全重新同步（或者从最近的备份进行恢复）。
- en: To avoid out-of-sync secondaries, it’s important to have a large oplog so that
    the primary can store a long history of operations. A larger oplog will obviously
    use more disk space, but in general this is a good tradeoff to make because disk
    space tends to be cheap and little of the oplog is typically in use, so it doesn’t
    take up much RAM. A general rule of thumb is that the oplog should provide coverage
    (replication window) for two to three days’ worth of normal operations. For more
    information on sizing the oplog, see [“Resizing the Oplog”](ch13.xhtml#sect1-resize-oplog).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免从节点不同步，有一个大的操作日志非常重要，这样主节点可以存储长时间的操作历史。一个较大的操作日志显然会使用更多的磁盘空间，但总体而言，这是一个很好的权衡，因为磁盘空间往往便宜，而且操作日志通常只有少量在使用，因此不会占用太多的内存。一个一般的经验法则是操作日志应该提供两到三天正常操作的覆盖范围（复制窗口）。有关调整操作日志大小的更多信息，请参阅[“调整操作日志大小”](ch13.xhtml#sect1-resize-oplog)。
- en: Heartbeats
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 心跳
- en: 'Members need to know about the other members’ states: who’s primary, who they
    can sync from, and who’s down. To keep an up-to-date view of the set, a member
    sends out a *heartbeat request* to every other member of the set every two seconds.
    A heartbeat request is a short message that checks everyone’s state.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 成员需要了解其他成员的状态：谁是主节点、可以从谁同步，以及谁宕机。为了保持集合的最新视图，每个成员每两秒向集合中的每个其他成员发送一个*心跳请求*。心跳请求是一个简短的消息，用于检查每个人的状态。
- en: One of the most important functions of heartbeats is to let the primary know
    if it can reach a majority of the set. If a primary can no longer reach a majority
    of the servers, it will demote itself and become a secondary (see [“How to Design
    a Set”](ch10.xhtml#repl-setup-section4)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 心跳的最重要功能之一是让主节点知道它是否能够联系到大多数集合成员。如果主节点不能再联系到大多数服务器，它将自动降级并成为从节点（参见[“如何设计一个集合”](ch10.xhtml#repl-setup-section4)）。
- en: Member States
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成员状态
- en: 'Members also communicate what state they are in via heartbeats. We’ve already
    discussed two states: primary and secondary. There are several other normal states
    that you’ll often see members be in:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 成员还通过心跳来传达它们的状态。我们已经讨论了两种状态：主节点和从节点。还有几种其他常见状态，你经常会看到成员处于其中：
- en: STARTUP
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 启动
- en: This is the state a member is in when it’s first started, while MongoDB is attempting
    to load its replica set configuration. Once the configuration has been loaded,
    it transitions to STARTUP2.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是成员在首次启动时的状态，此时 MongoDB 正在尝试加载其副本集配置。一旦加载了配置，它将转换到STARTUP2状态。
- en: STARTUP2
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: STARTUP2
- en: 'This state lasts throughout the initial sync process, which typically takes
    just a few seconds. The member forks off a couple of threads to handle replication
    and elections and then transitions into the next state: RECOVERING.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此状态持续整个初始同步过程，通常仅需几秒钟。成员分叉出几个线程来处理复制和选举，然后转入下一个状态：RECOVERING。
- en: RECOVERING
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: RECOVERING
- en: This state indicates that the member is operating correctly but is not available
    for reads. You may see it in a variety of situations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个状态表明成员操作正常，但不可用于读取。您可能会在各种情况下看到它。
- en: On startup, a member has to make a few checks to make sure it’s in a valid state
    before accepting reads; therefore, all members go through the RECOVERING state
    briefly on startup before becoming secondaries. A member can also go into this
    state during long-running operations such as compacting or in response to [the
    `replSetMaintenance` command](https://oreil.ly/6mJu-).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动时，成员必须进行一些检查，以确保其处于有效状态，然后才能接受读取操作；因此，在成为从属之前，所有成员在启动时都会短暂地经历RECOVERING状态。在长时间运行的操作（如压缩）期间或响应于[replSetMaintenance命令](https://oreil.ly/6mJu-)时，成员也可能进入此状态。
- en: A member will also go into the RECOVERING state if it has fallen too far behind
    the other members to catch up. This is, generally, a failure state that requires
    resyncing the member. The member does not go into an error state at this point
    because it lives in hope that someone will come online with a long-enough oplog
    that it can bootstrap itself back to non-staleness.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个成员落后于其他成员太多而无法追上，它也会进入RECOVERING状态。这通常是一个失败状态，需要重新同步该成员。此时成员不会进入错误状态，因为它依然希望某人在线上，有一个足够长的操作日志，可以使自己回到非陈旧状态。
- en: ARBITER
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ARBITER
- en: Arbiters (see [“Election Arbiters”](ch10.xhtml#electionArbiters)) have a special
    state and should always be in this state during normal operation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 仲裁者（参见[“选举仲裁者”](ch10.xhtml#electionArbiters)）具有特殊状态，在正常运行期间应始终处于此状态。
- en: 'There are also a few states that indicate a problem with the system. These
    include:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些状态表示系统存在问题。这些包括：
- en: DOWN
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DOWN
- en: If a member was up but then becomes unreachable, it will enter this state. Note
    that a member reported as “down” might, in fact, still be up, just unreachable
    due to network issues.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个成员曾经在线但后来变得无法访问，它将进入此状态。请注意，报告为“down”的成员实际上可能仍然在线，只是由于网络问题无法访问。
- en: UNKNOWN
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: UNKNOWN
- en: If a member has never been able to reach another member, it will not know what
    state it’s in, so it will report it as UNKNOWN. This generally indicates that
    the unknown member is down or that there are network problems between the two
    members.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个成员从未能够联系另一个成员，它将不知道自己处于什么状态，因此会报告为UNKNOWN。这通常表明未知成员已经宕机或两个成员之间存在网络问题。
- en: REMOVED
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: REMOVED
- en: This is the state of a member that has been removed from the set. If a removed
    member is added back into the set, it will transition back into its “normal” state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个从集合中删除的成员的状态。如果将已删除的成员重新添加到集合中，则它将重新转换为其“正常”状态。
- en: ROLLBACK
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ROLLBACK
- en: This state is used when a member is rolling back data, as described in [“Rollbacks”](#sect1-rollbacks).
    At the end of the rollback process, a server will transition back into the RECOVERING
    state and then become a secondary.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此状态用于成员回滚数据，如[“回滚”](#sect1-rollbacks)中所述。在回滚过程结束时，服务器将转换回RECOVERING状态，然后成为从属。
- en: Elections
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选举
- en: 'A member will seek election if it cannot reach a primary (and is itself eligible
    to become primary). A member seeking election will send out a notice to all of
    the members it can reach. These members may know why this member is an unsuitable
    primary: it may be behind in replication or there may already be a primary that
    the member seeking election cannot reach. In these cases, the other members will
    vote against the candidate.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个成员无法达到主节点（并且本身有资格成为主节点），它将寻求选举。寻求选举的成员会向其能够联系到的所有成员发送通知。这些成员可能知道为什么此成员不适合作为主节点：它可能在复制中落后，或者可能已经有一个主节点，而此成员无法达到。在这些情况下，其他成员将反对候选者。
- en: Assuming that there is no reason to object, the other members will vote for
    the member seeking election. If the member seeking election receives votes from
    a majority of the set, the election was successful and the member will transition
    into PRIMARY state. If it did not receive a majority if votes, it will remain
    a secondary and may try to become a primary again later. A primary will remain
    primary until it cannot reach a majority of members, goes down, or is stepped
    down, or the set is reconfigured.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 假设没有反对意见，其他成员将投票给寻求选举的成员。如果寻求选举的成员得到了大多数成员的投票，选举就成功了，该成员将过渡到主节点状态。如果没有获得多数票，它将保持次级状态，并可能稍后再次尝试成为主节点。主节点将保持主节点状态，直到无法与大多数成员通信、宕机、降级或设置重新配置。
- en: 'Assuming that the network is healthy and a majority of the servers are up,
    elections should be fast. It will take a member up to two seconds to notice that
    a primary has gone down (due to the heartbeats mentioned earlier) and it will
    immediately start an election, which should only take a few milliseconds. However,
    the situation is often nonoptimal: an election may be triggered due to networking
    issues or overloaded servers responding too slowly. In these cases, an election
    might take more time—even up to a few minutes.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设网络健康且大多数服务器正常运行，选举应该很快。一旦某个成员注意到主节点宕机（由之前提到的心跳引起），它会立即开始选举，整个过程应该只需要几毫秒。然而，情况通常不尽如人意：可能由于网络问题或过载服务器响应缓慢而触发选举。在这些情况下，选举可能需要更长时间——甚至可能达到几分钟。
- en: Rollbacks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回滚
- en: The election process described in the previous section means that if a primary
    does a write and goes down before the secondaries have a chance to replicate it,
    the next primary elected may not have the write. For example, suppose we have
    two data centers, one with the primary and a secondary, and the other with three
    secondaries, as shown in [Figure 11-2](#repl-misc-3).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节描述的选举过程中，如果一个主节点在次级节点有机会复制之前崩溃，下一个选出的主节点可能没有该写操作。例如，假设我们有两个数据中心，一个有主节点和一个次级节点，另一个有三个次级节点，如[图 11-2](#repl-misc-3)所示。
- en: '![](Images/mdb3_1102.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_1102.png)'
- en: Figure 11-2\. A possible two-data-center configuration
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-2\. 可能的两数据中心配置
- en: Suppose that there is a network partition between the two data centers, as shown
    in [Figure 11-3](#repl-misc-4). The servers in the first data center are up to
    operation 126, but that data center hasn’t yet replicated to the servers in the
    other data center.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设两个数据中心之间存在网络分区，如[图 11-3](#repl-misc-4)所示。第一个数据中心的服务器已经运行到操作 126，但该数据中心尚未将数据复制到另一个数据中心的服务器。
- en: '![](Images/mdb3_1103.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_1103.png)'
- en: Figure 11-3\. Replication across data centers can be slower than within a single
    data center
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. 跨数据中心的复制速度可能比单个数据中心内部慢
- en: The servers in the other data center can still reach a majority of the set (three
    out of five servers). Thus, one of them may be elected primary. This new primary
    begins taking its own writes, as shown in [Figure 11-4](#repl-misc-5).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个数据中心的服务器仍然可以与一半以上的服务器（五台中的三台）通信。因此，它们中的一个可能会被选为主节点。这个新的主节点开始接受自己的写操作，如[图 11-4](#repl-misc-5)所示。
- en: '![](Images/mdb3_1104.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_1104.png)'
- en: Figure 11-4\. Unreplicated writes won’t match writes on the other side of a
    network partition
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-4\. 未复制的写操作可能与网络分区另一侧的写操作不匹配
- en: When the network is repaired, the servers in the first data center will look
    for operation 126 to start syncing from the other servers, but will not be able
    to find it. When this happens, *A* and *B* will begin a process called *rollback*.
    Rollback is used to undo ops that were not replicated before failover. The servers
    with 126 in their oplogs will look back through the oplogs of the servers in the
    other data center for a common point. They’ll find that operation 125 is the latest
    operation that matches. [Figure 11-5](#fig-oplog-rollback) shows what the oplogs
    would look like. A apparently crashed before replicating ops 126−128, so these
    operations are not present on B, which has more recent operations. A will have
    to roll back these three operations before resuming syncing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当网络修复后，第一个数据中心中的服务器将查找操作 126 以开始从其他服务器同步，但找不到它。在这种情况下，*A* 和 *B* 将开始一个名为 *rollback*
    的进程。回滚用于撤消在故障转移前未复制的操作。其 oplogs 中具有操作 126 的服务器将回溯到另一个数据中心的服务器的 oplogs，寻找一个共同点。它们将发现操作
    125 是最新匹配的操作。[图 11-5](#fig-oplog-rollback) 显示了 oplogs 的情况。显然，A 在复制操作 126−128 之前崩溃了，因此这些操作不在
    B 上，而 B 具有更新的操作。A 在恢复同步之前必须回滚这三个操作。
- en: '![](Images/mdb3_1105.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_1105.png)'
- en: Figure 11-5\. Two members with conflicting oplogs⁠—the last common op was 125,
    so as B has more recent operations A will need to roll back ops 126-128
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-5\. 具有冲突操作日志的两个成员⁠—最后一个公共操作是 125，因此由于 B 具有更新操作，A 需要回滚操作 126-128。
- en: At this point, the server will go through the ops it has and write its version
    of each document affected by those ops to a *.bson* file in a *rollback* directory
    of your data directory. Thus, if (for example) operation 126 was an update, it
    will write the document updated by 126 to *<collectionName>.bson*. Then it will
    copy the version of that document from the current primary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时，服务器将检查其操作，并将受这些操作影响的每个文档的版本写入数据目录的 *rollback* 目录中的 *.bson* 文件中。因此，例如，如果操作
    126 是更新操作，则它将写入由 126 更新的文档到 *<collectionName>.bson*。然后，它将从当前主节点复制该文档的版本。
- en: 'The following is a paste of the log entries generated from a typical rollback:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成的典型回滚日志条目的粘贴：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The server begins syncing from another member (*server-1*, in this case) and
    realizes that it cannot find its latest operation on the sync source. At that
    point, it starts the rollback process by going into the ROLLBACK state (`replSet
    ROLLBACK`).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器开始从另一个成员（在本例中是 *server-1*）同步，并意识到在同步源上找不到其最新操作。此时，它通过进入 ROLLBACK 状态 (`replSet
    ROLLBACK`) 开始回滚过程。
- en: At step 2, it finds the common point between the two oplogs, which was 26 seconds
    ago. It then begins undoing the operations from the last 26 seconds from its oplog.
    Once the rollback is complete, it transitions into the RECOVERING state and begins
    syncing normally again.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 2 中，它找到了两个 oplogs 之间的共同点，即 26 秒前。然后，它开始撤消其 oplog 中最后 26 秒的操作。一旦回滚完成，它将过渡到
    RECOVERING 状态并开始正常同步。
- en: 'To apply operations that have been rolled back to the current primary, first
    use *mongorestore* to load them into a temporary collection:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要将已回滚的操作应用到当前主节点上，请首先使用 *mongorestore* 将它们加载到临时集合中：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then examine the documents (using the shell) and compare them to the current
    contents of the collection from whence they came. For example, if someone had
    created a “normal” index on the rollback member and a unique index on the current
    primary, you’d want to make sure that there weren’t any duplicates in the rolled-back
    data and resolve them if there were.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 shell 检查文档，并将其与它们所属的集合的当前内容进行比较。例如，如果有人在回滚成员上创建了“普通”索引，并在当前主键上创建了唯一索引，则需要确保回滚数据中没有重复项，并在有重复时解决它们。
- en: 'Once you have a version of the documents that you like in your staging collection,
    load it into your main collection:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您在暂存集合中获得满意的文档版本，请将其加载到主集合中：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you have any insert-only collections, you can directly load the rollback
    documents into the collection. However, if you are doing updates on the collection
    you will need to be more careful about how you merge rollback data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何仅支持插入的集合，可以直接将回滚文档加载到该集合中。但是，如果您在集合上执行更新操作，则需要更加注意如何合并回滚数据。
- en: One often-misused member configuration option is the number of votes each member
    has. Manipulating the number of votes is almost always not what you want to do
    and causes a lot of rollbacks (which is why it was not included in the list of
    member configuration options in the last chapter). Do not change the number of
    votes unless you are prepared to deal with regular rollbacks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经常被误用的成员配置选项是每个成员拥有的投票数。几乎总是不希望去操控投票数，并导致大量的回滚（这就是为什么它没有包含在上一章成员配置选项列表中）。除非你准备好处理常规回滚，否则不要更改投票数。
- en: For more information on preventing rollbacks, see [Chapter 12](ch12.xhtml#chapter-repl-app).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于预防回滚的信息，请参阅[第 12 章](ch12.xhtml#chapter-repl-app)。
- en: When Rollbacks Fail
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当回滚失败时
- en: In older versions of MongoDB, it could decide that the rollback was too large
    to undertake. Since MongoDB version 4.0, there is no limit on the amount of data
    that can be rolled back. A rollback in versions before 4.0 can fail if there are
    more than 300 MB of data or about 30 minutes of operations to roll back. In these
    cases, you must resync the node that is stuck in rollback.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧的 MongoDB 版本中，它可能会决定回滚的数据量太大而无法执行。自 MongoDB 版本 4.0 起，可以回滚的数据量没有限制。在 4.0 之前的版本中，如果有超过
    300 MB 的数据或大约 30 分钟的操作需要回滚，回滚可能会失败。在这些情况下，必须重新同步陷入回滚状态的节点。
- en: The most common cause of this is when secondaries are lagging and the primary
    goes down. If one of the secondaries becomes primary, it will be missing a lot
    of operations from the old primary. The best way to make sure you don’t get a
    member stuck in rollback is to keep your secondaries as up to date as possible.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况最常见的原因是当次要节点滞后而主节点宕机时。如果其中一个次要节点成为主节点，它将缺少旧主节点的许多操作。确保不会让成员陷入回滚的最佳方法是尽可能保持次要节点的最新状态。
