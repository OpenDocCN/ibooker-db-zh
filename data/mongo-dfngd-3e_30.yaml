- en: Chapter 24\. Deploying MongoDB
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第24章。部署 MongoDB
- en: 'This chapter gives recommendations for setting up a server to go into production.
    In particular, it covers:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了用于设置进入生产服务器的建议。具体来说，涵盖了：
- en: Choosing what hardware to buy and how to set it up
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择购买什么硬件以及如何设置它
- en: Using virtualized environments
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用虚拟化环境
- en: Important kernel and disk I/O settings
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的内核和磁盘 I/O 设置
- en: 'Network setup: who needs to connect to whom'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络设置：谁需要连接到谁
- en: Designing the System
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统设计
- en: You generally want to optimize for data safety and the quickest access you can
    afford. This section discusses the best way to accomplish these goals when choosing
    disks, RAID configuration, CPUs, and other hardware and low-level software components.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通常希望优化数据安全性和能够负担的最快访问速度。本节讨论在选择磁盘、RAID 配置、CPU 和其他硬件及低级软件组件时实现这些目标的最佳方法。
- en: Choosing a Storage Medium
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择存储介质
- en: 'In order of preference, we would like to store and retrieve data from:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 按照偏好顺序，我们希望从以下位置存储和检索数据：
- en: RAM
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAM
- en: SSD
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SSD
- en: Spinning disk
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 旋转磁盘
- en: Unfortunately, most people have limited budgets or enough data that storing
    everything in RAM is impractical and SSDs are too expensive. Thus, the typical
    deployment is a small amount of RAM (relative to total data size) and a lot of
    space on a spinning disk. If you are in this camp, the important thing is that
    your working set is smaller than RAM, and you should be ready to scale out if
    the working set gets bigger.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，大多数人的预算有限或者数据量足够大，使得将所有内容存储在 RAM 中不切实际，而 SSD 又太昂贵。因此，典型的部署是少量 RAM（相对于总数据量）和大量旋转磁盘空间。如果您处于这种情况，重要的是您的工作集小于
    RAM，并且如果工作集变大，您应该准备好进行扩展。
- en: If you are able to spend what you like on hardware, buy a lot of RAM and/or
    SSDs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能够在硬件上花费很多钱，购买大量 RAM 和/或 SSD。
- en: 'Reading data from RAM takes a few nanoseconds (say, 100). Conversely, reading
    from disk takes a few milliseconds (say, 10). It can be hard to picture the difference
    between these two numbers, so let’s scale them up to more relatable numbers: if
    accessing RAM took 1 second, accessing the disk would take over a day!'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从 RAM 读取数据需要几纳秒（比如说，100）。相反，从磁盘读取需要几毫秒（比如说，10）。很难想象这两个数字之间的差异，所以让我们将它们扩展到更容易理解的数字：如果访问
    RAM 需要 1 秒，那么访问磁盘将需要超过一天！
- en: 100 nanoseconds × 10,000,000 = 1 second
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 100 纳秒 × 10,000,000 = 1 秒
- en: 10 milliseconds × 10,000,000 = 1.16 days
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 10 毫秒 × 10,000,000 = 1.16 天
- en: These are very back-of-the-envelope calculations (your disk might be a bit faster
    or your RAM a bit slower), but the magnitude of this difference doesn’t change
    much. Thus, we want to access the disk as seldom as possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是非常粗略的估算（您的磁盘可能稍微快一些或者您的 RAM 可能稍微慢一些），但这种差异的数量级并不会有太大变化。因此，我们希望尽可能少地访问磁盘。
- en: Recommended RAID Configurations
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐的 RAID 配置
- en: RAID is hardware or software that lets you treat multiple disks as though they
    were a single disk. It can be used for reliability, performance, or both. A set
    of disks using RAID is referred to as a RAID array (somewhat redundantly, as RAID
    stands for redundant *array* of inexpensive disks).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RAID 是一种硬件或软件，可以让您将多个磁盘看作单个磁盘来使用。它可以用于可靠性、性能或两者兼而有之。使用 RAID 的一组磁盘称为 RAID 阵列（有点冗余，因为
    RAID 意为冗余的*廉价*磁盘阵列）。
- en: 'There are a number of ways to configure RAID, depending on the features you’re
    looking for—generally some combination of speed and fault tolerance. These are
    the most common varieties:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多配置 RAID 的方法，取决于您所寻找的功能组合——通常是一些速度和容错性的结合。以下是最常见的几种类型：
- en: RAID0
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RAID0
- en: Striping disks for improved performance. Each disk holds part of the data, similar
    to MongoDB’s sharding. Because there are multiple underlying disks, lots of data
    can be written to disk at the same time. This improves throughput on writes. However,
    if a disk fails and data is lost, there are no copies of it. It also can cause
    slow reads, as some data volumes may be slower than others.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高性能而进行磁盘条带化。每个磁盘保存部分数据，类似于 MongoDB 的分片。由于有多个底层磁盘，可以同时写入大量数据。这提高了写入的吞吐量。然而，如果一个磁盘故障并丢失数据，则没有其它副本。这也可能导致读取缓慢，因为某些数据卷可能比其他卷慢。
- en: RAID1
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RAID1
- en: Mirroring for improved reliability. An identical copy of the data is written
    to each member of the array. This has lower performance than RAID0, as a single
    member with a slow disk can slow down all writes. However, if a disk fails, you
    will still have a copy of the data on another member of the array.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可靠性而进行镜像。数据的完全副本被写入阵列的每个成员。这比RAID0具有较低的性能，因为一个速度较慢的磁盘会拖慢所有写操作。然而，如果一个磁盘故障，你仍然可以在阵列的另一个成员上找到数据的副本。
- en: RAID5
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RAID5
- en: Striping disks, plus keeping an extra piece of data about the other data that’s
    been stored to prevent data loss on server failure. Basically, RAID5 can handle
    one disk going down and hide that failure from the user. However, it is slower
    than any of the other varieties listed here because it needs to calculate this
    extra piece of information whenever data is written. This is particularly expensive
    with MongoDB, as a typical workload does many small writes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘条带化，以及保留关于已存储数据的其他数据片段，以防服务器故障时丢失数据。基本上，RAID5可以处理一个磁盘故障，并从用户那里隐藏此故障。然而，它比这里列出的任何其他变体都要慢，因为每次写入数据时都需要计算这个额外的信息。对于MongoDB来说尤其昂贵，因为典型的工作负载执行许多小写操作。
- en: RAID10
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RAID10
- en: 'A combination of RAID0 and RAID1: data is striped for speed and mirrored for
    reliability.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: RAID0和RAID1的结合：数据条带化以提高速度，同时镜像以提高可靠性。
- en: 'We recommend using RAID10: it is safer than RAID0 and can smooth out performance
    issues that can occur with RAID1\. However, some people feel that RAID1 on top
    of replica sets is overkill and opt for RAID0\. It is a matter of personal preference:
    how much risk are you willing to trade for performance?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用RAID10：它比RAID0更安全，并且可以解决RAID1可能出现的性能问题。然而，有些人认为在副本集之上使用RAID1过于保守，因此选择了RAID0。这是一个个人偏好的问题：你愿意为了性能而承担多大的风险？
- en: 'Do not use RAID5: it is very, very slow.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不要使用RAID5：它非常非常慢。
- en: CPU
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU
- en: MongoDB historically was very light on CPU, but with the use of the WiredTiger
    storage engine this is no longer the case. The WiredTiger storage engine is multithreaded
    and can take advantage of additional CPU cores. You should therefore balance your
    investment between memory and CPU.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB在历史上对CPU消耗很轻，但使用WiredTiger存储引擎后情况已经改变。WiredTiger存储引擎是多线程的，可以利用额外的CPU核心。因此，你应该在内存和CPU之间保持平衡。
- en: When choosing between speed and number of cores, go with speed. MongoDB is better
    at taking advantage of more cycles on a single processor than increased parallelization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在速度和核心数量之间进行选择时，请选择速度。MongoDB更擅长利用单个处理器上的更多周期，而不是增加并行性。
- en: Operating System
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作系统
- en: 64-bit Linux is the operating system MongoDB runs best on. If possible, use
    some flavor of that. CentOS and Red Hat Enterprise Linux are probably the most
    popular choices, but any flavor should work (Ubuntu and Amazon Linux are also
    common). Be sure to use the most recent stable version of the operating system,
    because old, buggy packages or kernels can sometimes cause issues.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 64位Linux是MongoDB表现最佳的操作系统。如果可能的话，请使用这些操作系统的某个变体。CentOS和Red Hat Enterprise Linux可能是最流行的选择，但任何变体都应该可以使用（Ubuntu和Amazon
    Linux也很常见）。请确保使用操作系统的最新稳定版本，因为旧的、有错误的软件包或内核有时可能会引起问题。
- en: 64-bit Windows is also well supported.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 64位Windows也得到了很好的支持。
- en: 'Other flavors of Unix are not as well supported: proceed with caution if you’re
    using Solaris or one of the BSD variants. Builds for these systems have, at least
    historically, had a lot of issues. MongoDB explicitly stopped supporting Solaris
    in August 2017, noting a lack of adoption among users.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其他Unix变体的支持不如Linux：如果你使用Solaris或BSD的其中一个，请谨慎处理。在历史上，这些系统的构建存在许多问题。MongoDB明确于2017年8月停止支持Solaris，指出用户对其缺乏采用。
- en: 'One important note on cross-compatibility: MongoDB uses the same wire protocol
    and lays out data files identically on all systems, so you can deploy on a combination
    of operating systems. For example, you could have a *mongos* process running on
    Windows and the *mongod*s that are its shards running on Linux. You can also copy
    data files from Windows to Linux or vice versa with no compatibility issues.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 关于跨兼容性的一项重要说明：MongoDB在所有系统上使用相同的传输协议并以相同的方式布置数据文件，因此你可以在多种操作系统组合上部署。例如，你可以在Windows上运行*mongos*进程，而其作为分片的*mongod*则在Linux上运行。你还可以在Windows和Linux之间复制数据文件，而无需担心兼容性问题。
- en: Since version 3.4, MongoDB no longer supports 32-bit x86 platforms. Do not run
    any type of MongoDB server on a 32-bit machine.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自版本3.4起，MongoDB不再支持32位x86平台。不要在32位机器上运行任何类型的MongoDB服务器。
- en: 'MongoDB works with little-endian architectures and one big-endian architecture:
    IBM’s zSeries. Most drivers support both little- and big-endian systems, so you
    can run clients on either. However, the server will typically be run on a little-endian
    machine.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 可与小端架构一起工作，并支持一种大端架构：IBM 的 zSeries。大多数驱动程序都支持小端和大端系统，因此您可以在任何一个上运行客户端。但服务器通常会在小端机器上运行。
- en: Swap Space
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交换空间
- en: You should allocate a small amount of swap in case memory limits are reached
    to prevent the kernel from killing MongoDB. It doesn’t usually use any swap space,
    but in extreme circumstances the WiredTiger storage engine might use some. If
    this occurs, then you should consider increasing the memory capacity of your machine
    or reviewing your workload to avoid this problematic situation for performance
    and for stability.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存限制达到时，应分配少量交换空间，以防止内核杀死 MongoDB。它通常不会使用任何交换空间，但在极端情况下，WiredTiger 存储引擎可能会使用一些。如果发生这种情况，则应考虑增加机器的内存容量或重新审视工作负载，以避免这种对性能和稳定性都具有问题的情况。
- en: 'The majority of memory MongoDB uses is “slippery”: it’ll be flushed to disk
    and replaced with other memory as soon as the system requests the space for something
    else. Therefore, database data should never be written to swap space: it’ll be
    flushed back to disk first.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 使用的大部分内存是“滑动”的：只要系统请求用于其他用途的空间，它就会被刷新到磁盘并用其他内存替换。因此，数据库数据不应该被写入交换空间：它将首先被刷新回磁盘。
- en: 'However, occasionally MongoDB will use swap for operations that require ordering
    data: either building indexes or sorting. It attempts not to use too much memory
    for these types of operations, but by performing many of them at the same time
    you may be able to force swapping.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，偶尔 MongoDB 会使用交换空间来进行需要排序数据的操作：无论是构建索引还是排序。它尽力不会为这些类型的操作使用过多内存，但通过同时执行许多这些操作，可能会强制进行交换。
- en: If your application is managing to make MongoDB use swap space, you should look
    into redesigning the application or reducing load on the swapping server.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的应用程序设法使 MongoDB 使用交换空间，应考虑重新设计应用程序或减少对交换服务器的负载。
- en: Filesystem
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件系统
- en: For Linux, only the XFS filesystem is recommended for your data volumes with
    the WiredTiger storage engine. It is possible to use the ext4 filesystem with
    WiredTiger, but be aware there are known performance issues (specifically, that
    it may stall on WiredTiger checkpoints).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Linux，仅推荐在使用 WiredTiger 存储引擎时使用 XFS 文件系统作为您的数据卷。虽然可以在 WiredTiger 上使用 ext4
    文件系统，但请注意已知的性能问题（特别是在 WiredTiger 检查点上可能会出现停顿）。
- en: On Windows, either NTFS or FAT is fine.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 上，NTFS 或 FAT 都可以。
- en: Warning
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Do not use Network File Storage (NFS) directly mounted for MongoDB storage.
    Some client versions lie about flushing, randomly remount and flush the page cache,
    and do not support exclusive file locking. Using NFS can cause journal corruption
    and should be avoided at all costs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不要直接将 Network File Storage（NFS）挂载用于 MongoDB 存储。某些客户端版本会关于刷新，随机重新挂载和刷新页面缓存，并不支持排他文件锁定。使用
    NFS 可能会导致日志损坏，应尽量避免。
- en: Virtualization
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化
- en: Virtualization is a great way to get cheap hardware and be able to expand fast.
    However, there are some downsides—particularly unpredictable network and disk
    I/O. This section covers virtualization-specific issues.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化是获取廉价硬件并能够快速扩展的好方法。然而，也存在一些缺点 —— 特别是不可预测的网络和磁盘 I/O。本节涵盖了虚拟化特定的问题。
- en: Memory Overcommitting
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存过度承诺
- en: 'The `memory overcommit` Linux kernel setting controls what happens when processes
    request too much memory from the operating system. Depending on how it’s set,
    the kernel may give memory to processes even if that memory is not actually available
    (in the hopes that it’ll become available by the time the process needs it). That’s
    called *overcommitting*: the kernel promises memory that isn’t actually there.
    This operating system kernel setting does not work well with MongoDB.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`memory overcommit` Linux 内核设置控制当进程从操作系统请求过多内存时会发生什么。根据设置方式，内核可能会向进程分配内存，即使该内存实际上并不可用（希望在进程需要时会变得可用）。这就是所谓的*过度承诺*：内核承诺了实际上并不存在的内存。这个操作系统内核设置与
    MongoDB 不兼容。'
- en: 'The possible values for `vm.overcommit_memory` are 0 (the kernel guesses about
    how much to overcommit); 1 (memory allocation always succeeds); or 2 (don’t commit
    more virtual address space than swap space plus a fraction of the overcommit ratio).
    The value 2 is complicated, but it’s the best option available. To set this, run:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`vm.overcommit_memory` 的可能值为 0（内核猜测超出量的大小）、1（内存分配总是成功）或 2（不要比交换空间加上超出比率的一小部分更多的虚拟地址空间）。值
    2 比较复杂，但是这是目前可用的最佳选项。要设置此选项，请运行：'
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You do not need to restart MongoDB after changing this operating system setting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 更改操作系统设置后，您无需重新启动 MongoDB。
- en: Mystery Memory
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神秘的内存
- en: Sometimes the virtualization layer does not handle memory provisioning correctly.
    Thus, you may have a virtual machine that claims to have 100 GB of RAM available
    but only ever allows you to access 60 GB of it. Conversely, we’ve seen people
    that were supposed to have 20 GB of memory end up being able to fit an entire
    100 GB dataset into RAM!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有时虚拟化层未正确处理内存分配。因此，您可能有一个虚拟机声称有 100GB 可用的内存，但实际上只能让您访问其中的 60GB。相反地，我们也看到有些人本应有
    20GB 内存，最终可以将整个 100GB 的数据集放入内存中！
- en: Assuming you don’t end up on the lucky side, there isn’t much you can do. If
    your operating system readahead is set appropriately and your virtual machine
    just won’t use all the memory it should, you may just have to switch virtual machines.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您没有运气，您将无能为力。如果您的操作系统预读设置正确，并且您的虚拟机只是不会使用应有的所有内存，您可能只能切换虚拟机。
- en: Handling Network Disk I/O Issues
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理网络磁盘 I/O 问题
- en: 'One of the biggest problems with using virtualized hardware is that you are
    generally sharing a disk with other tenants, which exacerbates the disk slowness
    mentioned previously because everyone is competing for disk I/O. Thus, virtualized
    disks can have very unpredictable performance: they can work fine while your neighbors
    aren’t busy and suddenly slow down to a crawl if someone else starts hammering
    the disks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用虚拟化硬件的最大问题之一是，通常您与其他租户共享磁盘，这会加剧之前提到的磁盘速度缓慢问题，因为每个人都在竞争磁盘 I/O。因此，虚拟化磁盘的性能非常不可预测：它们在您的邻居不忙的时候可能运行良好，但如果有人开始大量使用磁盘，它们可能突然变得运行缓慢。
- en: The other issue is that this storage is often not physically attached to the
    machine MongoDB is running on, so even when you have a disk all to yourself I/O
    will be slower than it would be with a local disk. There is also the unlikely-but-possible
    scenario of your MongoDB server losing its network connection to your data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，这种存储通常未直接连接到运行 MongoDB 的机器上，因此即使您拥有一块独立的硬盘，I/O 的速度也会比使用本地硬盘慢。还有一个不太可能但有可能的情况是，您的
    MongoDB 服务器与数据的网络连接中断。
- en: Amazon has what is probably the most widely used networked block store, called
    Elastic Block Store (EBS). EBS volumes can be connected to Elastic Compute Cloud
    (EC2) instances, allowing you to give a machine almost any amount of disk immediately.
    If you are using EC2, you should also enable AWS Enhanced Networking if it’s available
    for the instance type, as well as disable the dynamic voltage and frequency scaling
    (DVFS) and CPU power-saving modes plus hyperthreading. On the plus side, EBS makes
    backups very easy (take a snapshot from a secondary, mount the EBS drive on another
    instance, and start up *mongod*). On the downside, you may encounter variable
    performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊拥有可能是最广泛使用的网络块存储，称为弹性块存储（Elastic Block Store，EBS）。EBS 卷可以连接到弹性计算云（Elastic
    Compute Cloud，EC2）实例，允许您立即为机器提供几乎任意数量的磁盘。如果您正在使用 EC2，还应该在实例类型可用时启用 AWS 增强网络，并禁用动态电压和频率调节（DVFS）以及
    CPU 节能模式和超线程。从积极的一面来看，EBS 使备份变得非常简单（从次要源快照，将 EBS 驱动器挂载到另一个实例上，并启动 *mongod*）。但是，缺点是您可能会遇到性能不稳定的情况。
- en: If you require more predictable performance, there are a couple of options.
    One is to host MongoDB on your own servers—that way, you know no one else is slowing
    things down. However, that’s not an option for a lot of people, so the next best
    thing is to get an instance in the cloud that guarantees a certain number of I/O
    Operations Per Second (IOPS). See [*http://docs.mongodb.org*](http://docs.mongodb.org)
    for up-to-date recommendations on hosted offerings.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要更可预测的性能，有几个选择。一种是在自己的服务器上托管 MongoDB —— 这样，您就知道没有人会拖慢速度。但是，对于很多人来说，这并不是一个选择，因此下一个最好的选择就是在云中获取一个保证每秒
    I/O 操作数的实例。有关托管服务的最新建议，请参阅 [*http://docs.mongodb.org*](http://docs.mongodb.org)。
- en: If you can’t pursue either of these options and you need more disk I/O than
    an overloaded EBS volume can sustain, there is a way to hack around it. Basically,
    what you can do is keep monitoring the volume MongoDB is using. If and when that
    volume slows down, immediately kill that instance and bring up a new one with
    a different data volume.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能选择上述任何一种选项，而你需要更多的磁盘I/O超过过载的EBS卷能够支持的话，还有一种方法可以绕过它。基本上，你可以持续监控MongoDB正在使用的卷。如果那个卷变慢了，立即杀掉该实例，然后启动一个新的实例，并使用不同的数据卷。
- en: 'There are a couple of statistics to watch for:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个统计数据需要关注：
- en: Spiking I/O utilization (“IO wait” on Cloud Manager/Atlas), for obvious reasons.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I/O利用率飙升（在Cloud Manager/Atlas上的“IO wait”），显而易见的原因。
- en: 'Page fault rates spiking. Note that changes in application behavior could also
    cause working set changes: you should disable this assassination script before
    deploying new versions of your application.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面错误率飙升。请注意，应用程序行为的更改也可能导致工作集的变化：在部署新版本应用程序之前，你应该禁用这个刺杀脚本。
- en: 'The number of lost TCP packets going up (Amazon is particularly bad about this:
    when performance starts to fall, it drops TCP packets all over the place).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢失的TCP数据包数量上升（Amazon在这方面尤其糟糕：当性能开始下降时，它会随处丢弃TCP数据包）。
- en: MongoDB’s read and write queues spiking (this can be seen in Cloud Manager/Atlas
    or in *mongostat*’s `qr/qw` column).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB的读写队列飙升（可以在Cloud Manager/Atlas或*mongostat*的`qr/qw`列中看到）。
- en: 'If your load varies over the day or week, make sure your script takes that
    into account: you don’t want a rogue cron job killing off all of your instances
    because of an unusually heavy Monday morning rush.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的负载在一天或一周内有所变化，请确保你的脚本考虑到这一点：你不希望一个恶意的定时作业因为周一早晨的异常繁忙而杀掉所有的实例。
- en: 'This hack relies on you having recent backups or relatively quick-to-sync datasets.
    If you have each instance holding terabytes of data, you might want to pursue
    an alternative approach. Also, this is only *likely* to work: if your new volume
    is also being hammered, it will be just as slow as the old one.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧依赖于你有最近的备份或者数据集同步速度比较快。如果每个实例都持有几TB的数据，你可能需要考虑其他方法。此外，这只是*可能*有效：如果你的新卷也受到负载的影响，它将和旧卷一样慢。
- en: Using Non-Networked Disks
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用非网络连接的磁盘
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This section uses Amazon-specific vocabulary. However, it may apply to other
    providers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用了亚马逊特定的术语。然而，它可能适用于其他提供商。
- en: Ephemeral drives are the actual disks attached to the physical machine your
    VM is running on. They don’t have a lot of the problems networked storage does.
    Local disks can still be overloaded by other users on the same box, but with a
    large box you can be reasonably sure you’re not sharing disks with too many others.
    Even with a smaller instance, often an ephemeral drive will give better performance
    than a networked drive so long as the other tenants aren’t doing tons of IOPS.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 临时驱动器是连接到虚拟机所在物理机器的实际磁盘。它们没有网络存储的许多问题。本地磁盘仍然可能被同一台机器上的其他用户过载，但是在大型机器上，你可以合理地确保你不会与太多其他用户共享磁盘。即使是较小的实例，通常临时驱动器的性能也会比网络驱动器更好，只要其他租户不进行大量的IOPS操作。
- en: 'The downside is in the name: these disks are ephemeral. If your EC2 instance
    goes down, there’s no guarantee you’ll end up on the same box when you restart
    the instance, and then your data will be gone.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点正如其名：这些磁盘是临时的。如果你的EC2实例宕机，重新启动实例时不能保证会回到同一台机器，那么你的数据将会丢失。
- en: Thus, ephemeral drives should be used with care. You should make sure that you
    do not store any important or unreplicated data on these disks. In particular,
    do not put the journal on these ephemeral drives, or your database on network
    storage. In general, think of ephemeral drives as a slow cache rather than a fast
    disk and use them accordingly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，临时驱动器应谨慎使用。你应确保不在这些磁盘上存储任何重要或未复制的数据。特别是，不要将日志放在这些临时驱动器上，或者将你的数据库放在网络存储上。总体而言，把临时驱动器看作是一个慢速缓存而不是快速磁盘，并相应地使用它们。
- en: Configuring System Settings
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置系统设置
- en: There are several system settings that can help MongoDB run more smoothly, which
    are mostly related to disk and memory access. This section covers each of these
    options and how you should tweak them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个系统设置可以帮助MongoDB运行更顺畅，这些设置大多与磁盘和内存访问有关。本节涵盖了每个选项及其调整方法。
- en: Turning Off NUMA
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关闭NUMA
- en: When machines had a single CPU, all RAM was basically the same in terms of access
    time. As machines started to have more processors, engineers realized that having
    all memory be equally far from each CPU (as shown in [Figure 24-1](#ops311)) was
    less efficient than having each CPU have some memory that is especially close
    to it and fast for that particular CPU to access ([Figure 24-2](#ops312)). This
    architecture, where each CPU has its own “local” memory, is called *nonuniform
    memory architecture* (NUMA).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算机只有一个 CPU 时，所有 RAM 在访问时间上基本相同。随着计算机开始拥有更多处理器，工程师意识到让所有内存与每个 CPU 等距离（如 [图 24-1](#ops311)
    所示）不如让每个 CPU 有一些特别靠近它的内存，并且对该 CPU 访问快速的内存（如 [图 24-2](#ops312) 所示）更有效率。每个 CPU 都有其自己“本地”内存的这种架构称为*非统一内存架构*（NUMA）。
- en: '![](Images/mdb3_2401.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_2401.png)'
- en: 'Figure 24-1\. Uniform memory architecture: all memory has the same access cost
    for each CPU'
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 24-1\. 统一内存架构：每个 CPU 对所有内存的访问成本相同
- en: '![](Images/mdb3_2402.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_2402.png)'
- en: 'Figure 24-2\. Nonuniform memory architecture: certain memory is attached to
    a CPU, giving the CPU faster access to that memory; CPUs can still access other
    CPUs’ memory, but it is more expensive than accessing their own'
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 24-2\. 非统一内存架构：某些内存连接到一个 CPU，使该 CPU 更快地访问该内存；CPU 仍然可以访问其他 CPU 的内存，但与访问自己的内存相比，成本更高
- en: 'For lots of applications, NUMA works well: the processors often need different
    data because they’re running different programs. However, this works terribly
    for databases in general and MongoDB in particular because databases have such
    different memory access patterns than other types of applications. MongoDB uses
    a massive amount of memory and needs to be able to access memory that is “local”
    to other CPUs. However, the default NUMA settings on many systems make this difficult.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用程序来说，NUMA 的效果很好：处理器通常需要不同的数据，因为它们运行不同的程序。然而，这对于数据库来说效果极差，特别是 MongoDB，因为数据库的内存访问模式与其他类型的应用程序大不相同。MongoDB
    使用大量内存，并且需要能够访问“本地”于其他 CPU 的内存。然而，许多系统上默认的 NUMA 设置使得这一点很困难。
- en: CPUs favor using the memory that is attached to them, and processes tend to
    favor one CPU over the others. This means that memory often fills up unevenly,
    potentially leaving you with one processor using 100% of its local memory and
    the other processors using only a fraction of their memory, as shown in [Figure 24-3](#ops313).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: CPUs 偏向使用与它们连接的内存，而进程倾向于优先使用一个 CPU。这意味着内存通常会不均匀地填满，可能导致一个处理器使用其本地内存的 100%，而其他处理器仅使用它们内存的一小部分，如
    [图 24-3](#ops313) 所示。
- en: '![](Images/mdb3_2403.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_2403.png)'
- en: Figure 24-3\. Sample memory usage in a NUMA system
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 24-3\. NUMA 系统中的示例内存使用情况
- en: In the scenario in [Figure 24-3](#ops313), suppose CPU1 needs some data that
    isn’t in memory yet. It must use its local memory for data that doesn’t have a
    “home” yet, but its local memory is full. Thus, it has to evict some of the data
    in its local memory to make room for the new data, even though there’s plenty
    of space left in the memory attached to CPU2! This process tends to cause MongoDB
    to run much slower than expected, as it only has a fraction of the memory available
    that it should have. MongoDB vastly prefers semiefficient access to more data
    over extremely efficient access to less data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 24-3](#ops313) 的场景中，假设 CPU1 需要一些尚未在内存中的数据。它必须使用其本地内存存储没有“归属”的数据，但其本地内存已满。因此，它必须逐出一些本地内存中的数据，为新数据腾出空间，即使
    CPU2 的内存还有大量空间可用！这个过程往往会导致 MongoDB 运行速度远低于预期，因为它只能使用可用内存的一小部分。MongoDB 更倾向于半有效地访问更多数据，而不是极其有效地访问更少的数据。
- en: When running MongoDB servers and clients on NUMA hardware, you should configure
    a memory interleave policy so that the host behaves in a non-NUMA fashion. MongoDB
    checks NUMA settings on startup when deployed on Linux and Windows machines. If
    the NUMA configuration may degrade performance, MongoDB prints a warning.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NUMA 硬件上运行 MongoDB 服务器和客户端时，应配置内存交织策略，以使主机以非 NUMA 方式运行。MongoDB 在 Linux 和 Windows
    机器上部署时会检查 NUMA 设置。如果 NUMA 配置可能降低性能，MongoDB 将打印警告。
- en: On Windows, memory interleaving must be enabled through the machine’s BIOS.
    Consult your system documentation for details.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 上，必须通过机器的 BIOS 启用内存交织。请参考系统文档获取详细信息。
- en: 'When running MongoDB on Linux, you should disable zone reclaim in the *sysctl*
    settings using one of the following commands:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Linux 上运行 MongoDB 时，应使用以下命令之一在 *sysctl* 设置中禁用区域回收：
- en: '`echo 0 | sudo tee /proc/sys/vm/zone_reclaim_mode`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`echo 0 | sudo tee /proc/sys/vm/zone_reclaim_mode`'
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, you should use *numactl* to start your *mongod* instances, including the
    config servers, *mongos* instances, and any clients. If you do not have the `numactl`
    command, refer to the documentation for your operating system to install the *numactl*
    package.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您应该使用 *numactl* 启动您的 *mongod* 实例，包括配置服务器、*mongos* 实例和任何客户端。如果您没有 `numactl`
    命令，请参阅您操作系统的文档安装 *numactl* 软件包。
- en: 'The following command demonstrates how to start a MongoDB instance using *numactl*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的命令演示了如何使用 *numactl* 启动 MongoDB 实例：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The *`<path>`* is the path to the program you are starting and the *`<options>`*
    are any optional arguments to pass to the program.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*`<路径>`* 是您要启动的程序的路径，而 *`<选项>`* 是要传递给该程序的任何可选参数。'
- en: To fully disable NUMA behavior, you must perform both operations. For more information,
    see the [documentation](https://oreil.ly/cm-_D).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全禁用NUMA行为，您必须执行这两个操作。有关更多信息，请参阅[文档](https://oreil.ly/cm-_D)。
- en: Setting Readahead
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置预读
- en: 'Readahead is an optimization where the operating system reads more data from
    disk than was actually requested. This is useful because most workloads that computers
    handle are sequential: if you load the first 20 MB of a video, you are probably
    going to want the next couple of megabytes of it. Thus, the system will read more
    from disk than you actually request and store it in memory, just in case you need
    it soon.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 预读是操作系统读取比实际请求的数据更多的优化方法。这是有用的，因为大多数计算机处理的工作负载是顺序的：如果您加载视频的前 20 MB，您可能会希望获取接下来的几兆字节。因此，系统将从磁盘读取比您实际请求的更多数据，并将其存储在内存中，以防您很快需要它。
- en: For the WiredTiger storage engine, you should set readahead to between 8 and
    32 regardless of the storage media type (spinning disk, SSD, etc.). Setting it
    higher benefits sequential I/O operations, but since MongoDB disk access patterns
    are typically random, a higher readahead value provides limited benefit and may
    even result in performance degradation. For most workloads, a readahead of between
    8 and 32 provides optimal MongoDB performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 WiredTiger 存储引擎，无论存储介质类型（旋转磁盘、SSD 等），都应将预读设置为 8 到 32。将其设置得更高有助于顺序 I/O 操作，但由于
    MongoDB 的磁盘访问模式通常是随机的，较高的预读值提供的益处有限，甚至可能导致性能下降。对于大多数工作负载，预读设置为 8 到 32 提供了最佳的 MongoDB
    性能。
- en: In general, you should set the readahead within this range unless testing shows
    that a higher value is measurably, repeatably, and reliably beneficial. MongoDB
    Professional Support can provide advice and guidance on nonzero readahead configurations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，您应该在此范围内设置预读（readahead），除非测试表明更高的值在可测性、重复性和可靠性方面都有明显的益处。MongoDB 专业支持可以就非零预读配置提供建议和指导。
- en: Disabling Transparent Huge Pages (THP)
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用透明大页（THP）
- en: 'THP causes similar issues to high readahead. Do not use this feature unless:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: THP 会引起类似于高预读的问题。除非
- en: All of your data fits into memory.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的所有数据都适合放入内存。
- en: You have no plans for it to ever grow beyond memory.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您没有计划让它超出内存范围。
- en: MongoDB needs to page in lots of tiny pieces of memory, so using THP can result
    in more disk I/O.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 需要将大量小片段的内存分页，因此使用 THP 可能会导致更多的磁盘 I/O。
- en: Systems move data from disk to memory and back by the page. Pages are generally
    a couple of kilobytes (x86 defaults to 4,096-byte pages). If a machine has many
    gigabytes of memory, keeping track of each of these (relatively tiny) pages can
    be slower than just tracking a few larger-granularity pages. THP is a solution
    that allows you to have pages that are up to 256 MB (on IA-64 architectures).
    However, using it means that you are keeping megabytes of data from one section
    of disk in memory. If your data does not fit in RAM, then swapping in larger pieces
    from disk will just fill up your memory quickly with data that will need to be
    swapped out again. Also, flushing any changes to disk will be slower, as the disk
    must write megabytes of “dirty” data, instead of a few kilobytes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 系统通过页面从磁盘到内存再回来移动数据。页面通常是几千字节（x86 默认为 4,096 字节页面）。如果一台机器有许多吉字节的内存，追踪每个（相对较小的）页面可能比仅追踪几个更大粒度的页面更慢。THP
    是一个解决方案，允许您拥有高达 256 MB 的页面（适用于 IA-64 架构）。但是，使用它意味着您正在将来自磁盘一部分的兆字节数据保留在内存中。如果您的数据不适合在
    RAM 中，则从磁盘中交换更大的数据块将快速填满您的内存，需要再次交换出去。此外，刷新任何更改到磁盘的速度会更慢，因为磁盘必须写入兆字节的“脏”数据，而不是几千字节。
- en: THP was actually developed to benefit databases, so this may be surprising to
    experienced database admins. However, MongoDB tends to do a lot less sequential
    disk access than relational databases do.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: THP实际上是为了使数据库受益而开发的，因此对经验丰富的数据库管理员来说可能会感到惊讶。 然而，MongoDB往往比关系数据库进行较少的顺序磁盘访问。
- en: Note
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: On Windows these are called Large Pages, not Huge Pages. Some versions of Windows
    have this feature enabled by default and some do not, so check and make sure it
    is turned off.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上，这些称为大页而不是巨大页。 某些Windows版本默认启用此功能，某些则没有，因此请检查并确保已关闭。
- en: Choosing a Disk Scheduling Algorithm
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择磁盘调度算法
- en: The disk controller receives requests from the operating system and processes
    them in an order determined by a scheduling algorithm. Sometimes changing this
    algorithm can improve disk performance. For other hardware and workloads, it may
    not make a difference. The best way to decide which algorithm to use is to test
    them out yourself on your workload. Deadline and completely fair queueing (CFQ)
    both tend to be good choices.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘控制器从操作系统接收请求，并根据调度算法确定的顺序处理它们。 有时更改此算法可以改善磁盘性能。 对于其他硬件和工作负载，可能没有区别。 决定使用哪种算法的最佳方法是在您自己的工作负载上进行测试。
    截止时间和完全公平排队（CFQ）通常都是不错的选择。
- en: There are a couple of situations where the noop scheduler (a contraction of
    “no-op”) is the best choice. If you’re in a virtualized environment, use the noop
    scheduler. This scheduler basically passes the operations through to the underlying
    disk controller as quickly as possible. It is fastest to do this and let the real
    disk controller handle any reordering that needs to happen.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种情况下，noop调度器（“no-op”的缩写）是最佳选择。 如果您处于虚拟化环境中，请使用noop调度器。 此调度程序基本上将操作尽快传递到底层磁盘控制器。
    最快的方法是这样做，并让真实的磁盘控制器处理任何需要进行的重新排序。
- en: Similarly, on SSDs, the noop scheduler is generally the best choice. SSDs don’t
    have the same locality issues that spinning disks do.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在SSD上，noop调度器通常是最佳选择。 SSD没有与旋转磁盘相同的局部性问题。
- en: Finally, if you’re using a RAID controller with caching, use noop. The cache
    behaves like an SSD and will take care of propagating the writes to the disk efficiently.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您使用带有缓存的RAID控制器，请使用noop。 缓存的行为类似于SSD，将有效地将写操作传播到磁盘。
- en: If you are on a physical server that is not virtualized, the operating system
    should use the deadline scheduler. The deadline scheduler caps maximum latency
    per request and maintains a reasonable disk throughput that is best for disk-intensive
    database applications.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在未虚拟化的物理服务器上，则操作系统应使用截止时间调度程序。 截止时间调度程序限制每个请求的最大延迟，并保持对于磁盘密集型数据库应用程序最佳的合理磁盘吞吐量。
- en: You can change the scheduling algorithm by setting the `--elevator` option in
    your boot configuration.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在引导配置中设置`--elevator`选项来更改调度算法。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The option is called "elevator" because the scheduler behaves like an elevator,
    picking up people (I/O requests) from different floors (processes/times) and dropping
    them off where they want to go in an arguabley optimal way.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选项称为“电梯”，因为调度器的行为类似于电梯，从不同的楼层（进程/时间）中接收人们（I/O请求），并以一种较优的方式将它们送到它们想去的地方。
- en: Often all of the algorithms work pretty well; you may not see much of a difference
    between them.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常所有的算法表现都很好； 您可能看不出它们之间有多大区别。
- en: Disabling Access Time Tracking
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用访问时间跟踪
- en: 'By default, the system tracks when files were last accessed. As the data files
    used by MongoDB are very high-traffic, you can get a performance boost by disabling
    this tracking. You can do this on Linux by changing `atime` to `noatime` in */etc/fstab*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，系统会跟踪文件上次访问的时间。 由于MongoDB使用的数据文件非常频繁，因此通过禁用此跟踪可以提高性能。 您可以在Linux上通过在*/etc/fstab*中将`atime`更改为`noatime`来执行此操作：
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You must remount the device for the changes to take effect.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须重新挂载设备以使更改生效。
- en: '`atime` is more of an issue on older kernels (e.g., ext3); newer ones use `relatime`
    as a default, which is less aggressively updated. Also, be aware that setting
    `noatime` can affect other programs using the partition, such as *mutt* or backup
    tools.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在较旧的内核（例如ext3）上，`atime`是一个更大的问题； 较新的内核默认使用`relatime`，它的更新较少。 另外，请注意，设置`noatime`可能会影响使用该分区的其他程序，例如*mutt*或备份工具。
- en: 'Similarly, on Windows you should set the `disablelastaccess` option. To turn
    off last access time recording, run:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在Windows上，您应该设置`disablelastaccess`选项。 要关闭最后访问时间记录，请运行：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You must reboot for this setting to take effect. Setting this may affect the
    remote storage service, but you probably shouldn’t be using a service that automatically
    moves your data to other disks anyway.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 必须重新启动才能使此设置生效。设置这个可能会影响远程存储服务，但您可能不应该使用自动将数据移动到其他磁盘的服务。
- en: Modifying Limits
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改限制
- en: 'There are two limits that MongoDB tends to blow by: the number of threads a
    process is allowed to spawn and the number of file descriptors a process is allowed
    to open. Both of these should generally be set to unlimited.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB往往会超出两个限制：一个进程允许生成的线程数和一个进程允许打开的文件描述符数。通常情况下，这两者都应该设置为无限制。
- en: Whenever a MongoDB server accepts a connection, it spawns a thread to handle
    all activity on that connection. Therefore, if you have 3,000 connections to the
    database, the database will have 3,000 threads running (plus a few other threads
    for non-client-related tasks). Depending on your application server configuration,
    your client may spawn anywhere from a dozen to thousands of connections to MongoDB.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每当MongoDB服务器接受一个连接时，它会生成一个线程来处理该连接上的所有活动。因此，如果您有3000个连接到数据库的连接，数据库将有3000个正在运行的线程（加上一些用于非客户端相关任务的其他线程）。根据您的应用服务器配置，您的客户端可能会生成从几十到数千个连接到MongoDB的连接。
- en: If your client will dynamically spawn more child processes as traffic increases
    (most application servers will do this), it is important to make sure that these
    child processes are not so numerous that they can max out MongoDB’s limits. For
    example, if you have 20 application servers, each one of which is allowed to spawn
    100 child processes, and each child process can spawn 10 threads that all connect
    to MongoDB, that could result in the spawning of 20 × 100 × 10 = 20,000 connections
    at peak traffic. MongoDB is probably not going to be very happy about spawning
    tens of thousands of threads and, if you run out of threads per process, will
    simply start refusing new connections.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的客户端会随着流量增加动态生成更多的子进程（大多数应用服务器会这样做），重要的是确保这些子进程数量不要太多，以至于它们可以超出MongoDB的限制。例如，如果您有20个应用服务器，每个服务器允许生成100个子进程，并且每个子进程可以生成10个线程，所有这些都连接到MongoDB，那么在高峰时段可能会生成20
    × 100 × 10 = 20000个连接。MongoDB可能不会很高兴地生成数万个线程，并且如果您的进程线程数达到上限，将简单地开始拒绝新的连接。
- en: The other limit to modify is the number of file descriptors MongoDB is allowed
    to open. Every incoming and outgoing connection uses a file descriptor, so the
    client connection storm just mentioned would create 20,000 open filehandles.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要修改的限制是MongoDB允许打开的文件描述符数。每个传入和传出的连接都使用一个文件描述符，因此刚才提到的客户端连接风暴将创建20000个打开的文件句柄。
- en: '*mongos* in particular tends to create connections to many shards. When a client
    connects to a *mongos* and makes a request, the *mongos* opens connections to
    any and all shards necessary to fulfill that request. Thus, if a cluster has 100
    shards and a client connects to a *mongos* and tries to query for all of its data,
    the *mongos* must open 100 connections: one connection to each shard. This can
    quickly lead to an explosion in the number of connections, as you can imagine
    from the previous example. Suppose a liberally configured app server made a hundred
    connections to a *mongos* process. This could get translated to 100 inbound connections
    × 100 shards = 10,000 connections to shards! (This assumes a nontargeted query
    on each connection, which would be a bad design, so this is a somewhat extreme
    example.)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是*mongos*倾向于创建到许多分片的连接。当客户端连接到*mongos*并发出请求时，*mongos*会打开到所有必要的分片的连接来完成该请求。因此，如果一个集群有100个分片，并且客户端连接到*mongos*并尝试查询其所有数据，则*mongos*必须打开100个连接：每个分片一个连接。这可能很快导致连接数量激增，正如前面的例子所示。假设一个配置很宽松的应用服务器向*mongos*进程创建了100个连接。这可能被翻译为100个入站连接
    × 100个分片 = 10000个分片的连接！（这假设每个连接上都有一个非目标化的查询，这是一个很糟糕的设计，因此这是一个相对极端的例子。）
- en: Thus, there are a few adjustments to make. Many people purposefully configure
    *mongos* processes to only allow a certain number of incoming connections by using
    the `maxConns` option. This is a good way to enforce that your client is behaving
    well.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要进行一些调整。许多人有意配置*mongos*进程，仅允许使用`maxConns`选项来限制特定数量的传入连接。这是强制确保客户端行为良好的好方法。
- en: You should also increase the limit on the number of file descriptors, as the
    default (generally 1,024) is simply too low. Set the max number of file descriptors
    to [unlimited](https://oreil.ly/oTGLL) or, if you’re nervous about that, 20,000\.
    Each system has a different way of changing these limits, but in general, make
    sure that you change both the hard and soft limits. A hard limit is enforced by
    the kernel and can only be changed by an administrator, whereas a soft limit is
    user-configurable.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应增加文件描述符数量限制，因为默认值（通常为 1,024）太低了。将最大文件描述符设置为[无限](https://oreil.ly/oTGLL)，或者如果对此感到不安，设置为
    20,000。每个系统更改这些限制的方式不同，但通常确保同时更改硬限制和软限制。硬限制由内核强制执行，只能由管理员更改，而软限制是可由用户配置的。
- en: If the maximum number of connections is left at 1,024, Cloud Manager will warn
    you by displaying the host in yellow in the host list. If low limits are the issue
    that triggered the warning, the Last Ping tab should display a message similar
    to that shown in [Figure 24-4](#ops351).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将最大连接数保留为 1,024，Cloud Manager 将通过在主机列表中以黄色显示主机来警告您。如果低限制是触发警告的原因，则“最后 ping”选项卡应显示与[图 24-4](#ops351)类似的消息。
- en: '![](Images/mdb3_2404.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/mdb3_2404.png)'
- en: Figure 24-4\. Cloud Manager low ulimit (file descriptors) setting warning
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 24-4。Cloud Manager 低 ulimit（文件描述符）设置警告
- en: Even if you have a nonsharded setup and an application that only uses a small
    number of connections, it’s a good idea to increase the hard and soft limits to
    at least 4,096\. That will stop MongoDB from warning you about them and give you
    some breathing room, just in case.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您采用非分片设置并且应用程序仅使用少量连接，将硬限制和软限制至少增加到 4,096 是个好主意。这将阻止 MongoDB 提示您有关这些问题，并为您提供某些操作余地以防万一。
- en: Configuring Your Network
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置您的网络
- en: This section covers which servers should have connectivity to which other servers.
    Often, for reasons of network security (and sensibility), you may want to limit
    the connectivity of MongoDB servers. Note that multiserver MongoDB deployments
    should handle networks being partitioned or down, but it isn’t recommended as
    a general deployment strategy.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了哪些服务器应与其他服务器建立连接的内容。通常基于网络安全（和合理性）的原因，您可能希望限制 MongoDB 服务器的连接性。请注意，多服务器
    MongoDB 部署应处理网络被分割或宕机的情况，但这并不推荐作为一般部署策略。
- en: For a standalone server, clients must be able to make connections to the *mongod*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于独立服务器，客户端必须能够连接到*mongod*。
- en: Members of a replica set must be able to make connections to every other member.
    Clients must be able to connect to all nonhidden, nonarbiter members. Depending
    on network configuration, members may also attempt to connect to themselves, so
    you should allow *mongod*s to create connections to themselves.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 副本集的成员必须能够与其他每个成员建立连接。客户端必须能够连接到所有非隐藏、非仲裁成员。根据网络配置，成员也可能尝试连接自己，因此应允许*mongod*之间创建连接。
- en: 'Sharding is a bit more complicated. There are four components: *mongos* servers,
    shards, config servers, and clients. Connectivity can be summarized in the following
    three points:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 分片稍微复杂。它包括四个组件：*mongos*服务器、分片、配置服务器和客户端。连接性可以总结为以下三点：
- en: A client must be able to connect to a *mongos*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端必须能够连接到*mongos*。
- en: A *mongos* must be able to connect to the shards and config servers.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*mongos*必须能够连接到分片和配置服务器。'
- en: A shard must be able to connect to the other shards and the config servers.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分片必须能够连接到其他分片和配置服务器。
- en: The full connectivity chart is described in [Table 24-1](#table23-1).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的连接图表详见[表 24-1](#table23-1)。
- en: Table 24-1\. Sharding connectivity
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表 24-1。分片连接性
- en: '| Connectivity | from server type |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 连接性 | 来自服务器类型 |'
- en: '| --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| to server type | *mongos* | Shard | Config server | Client |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 到服务器类型 | *mongos* | 分片 | 配置服务器 | 客户端 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| *mongos* | Not required | Not required | Not required | Required |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| *mongos* | 不必需 | 不必需 | 不必需 | 必需 |'
- en: '| Shard | Required | Required | Not required | Not recommended |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 分片 | 必需 | 必需 | 不必需 | 不建议 |'
- en: '| Config server | Required | Required | Not required | Not recommended |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 配置服务器 | 必需 | 必需 | 不必需 | 不建议 |'
- en: '| Client | Not required | Not required | Not required | Not MongoDB-related
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 客户端 | 不必需 | 不必需 | 不必需 | 与 MongoDB 无关 |'
- en: There are three possible values in the table. “Required” means that connectivity
    between these two components is required for sharding to work as designed. MongoDB
    will attempt to degrade gracefully if it loses these connections due to network
    issues, but you shouldn’t purposely configure it that way.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表中有三个可能的值。“必需”意味着这两个组件之间的连接对于分片按设计工作是必要的。由于网络问题，如果丢失这些连接，MongoDB将尝试优雅降级，但不应故意配置成这样。
- en: “Not required” means that these two elements never talk in the direction specified,
    so no connectivity is needed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: “不需要”意味着这两个元素从未按指定方向通信，因此不需要连接。
- en: “Not recommended” means that these two elements should never talk, but due to
    user error they could. For example, it is recommended that clients only make connections
    to the *mongos*, not the shards, so that clients do not inadvertently make requests
    directly to shards. Similarly, clients should not be able to directly access config
    servers so that they cannot accidentally modify config data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: “不推荐”意味着这两个元素不应该通信，但由于用户错误可能会发生。例如，建议客户端只连接到*mongos*，而不是分片，这样客户端就不会不经意地直接向分片发出请求。同样，客户端不应直接访问配置服务器，以防止意外修改配置数据。
- en: Note that *mongos* processes and shards talk to config servers, but config servers
    don’t make connections to anyone, even one another.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*mongos* 进程和分片与配置服务器通信，但配置服务器不与任何人建立连接，甚至彼此也不例外。
- en: 'Shards must communicate during migrates: shards connect to one another directly
    to transfer data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 分片在迁移过程中必须进行通信：分片直接连接到彼此以传输数据。
- en: As mentioned earlier, replica set members that compose shards should be able
    to connect to themselves.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，组成分片的副本集成员应能够连接到自身。
- en: System Housekeeping
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统维护
- en: This section covers some common issues you should be aware of before deploying.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了部署前应注意的一些常见问题。
- en: Synchronizing Clocks
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 同步时钟
- en: In general, it’s safest to have your systems’ clocks within a second of each
    other. Replica sets should be able to handle nearly any clock skew. Sharding can
    handle some skew (if it gets beyond a few minutes, you’ll start seeing warnings
    in the logs), but it’s best to minimize it. Having in-sync clocks also makes figuring
    out what’s happening from logs easier.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，确保系统时钟相差不超过一秒是最安全的。副本集应能处理几乎任何时钟偏差。分片可以处理一些偏差（如果超过几分钟，你将在日志中看到警告），但最好将其最小化。保持时钟同步还能更轻松地从日志中了解发生的情况。
- en: You can keep clocks synchronized using the *w32tm* tool on Windows and the *ntp*
    daemon on Linux.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Windows上的*w32tm*工具和Linux上的*ntp*守护进程来保持时钟同步。
- en: The OOM Killer
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OOM Killer
- en: Very occasionally, MongoDB will allocate enough memory that it will be targeted
    by the out-of-memory (OOM) killer. This particularly tends to happen during index
    builds, as that is one of the only times when MongoDB’s resident memory should
    put any strain on the system.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 非常偶尔，MongoDB会分配足够的内存，以至于会被内存不足（OOM）杀手瞄准。这通常发生在索引构建期间，因为这是MongoDB的驻留内存可能对系统造成压力的几个时刻之一。
- en: If your MongoDB process suddenly dies with no errors or exit messages in the
    logs, check */var/log/messages* (or wherever your kernel logs such things) to
    see if it has any messages about terminating *mongod*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的MongoDB进程突然死机，并且日志中没有任何错误或退出消息，请检查*/var/log/messages*（或者其他记录这类信息的位置），看看是否有关于终止*mongod*的消息。
- en: 'If the kernel has killed MongoDB for memory overuse, you should see something
    like this in the kernel log:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果内核因内存过度使用而杀死了MongoDB进程，你应该在内核日志中看到类似以下的信息：
- en: '[PRE5]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you were running with journaling, you can simply restart *mongod* at this
    point. If you were not, restore from a backup or resync the data from a replica.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用了日志记录，此时可以简单地重新启动*mongod*。如果没有，从备份中恢复数据或从副本重新同步数据。
- en: The OOM killer gets particularly nervous if you have no swap space and start
    running low on memory, so a good way to prevent it from going on a spree is to
    configure a modest amount of swap. As mentioned earlier, MongoDB should never
    use it, but it makes the OOM killer happy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有交换空间并且内存不足，OOM Killer会变得特别紧张，因此防止它胡作非为的一个好方法是配置适量的交换空间。正如前面提到的，MongoDB永远不应该使用它，但这会让OOM
    Killer感到满意。
- en: If the OOM killer kills a *mongos*, you can simply restart it.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果OOM Killer杀死了一个*mongos*，你可以简单地重新启动它。
- en: Turn Off Periodic Tasks
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关闭定期任务
- en: Check that there aren’t any cron jobs, antivirus scanners, or daemons that might
    periodically pop to life and steal resources. One culprit we’ve seen is package
    managers’ automatic update. These programs will come to life, consume a ton of
    RAM and CPU, and then disappear. This is not something you want running on your
    production server.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 检查没有任何可能定期启动并占用资源的 cron 作业、杀毒软件扫描器或守护进程。我们见过的一个罪魁祸首是包管理器的自动更新。这些程序会突然启动，消耗大量的
    RAM 和 CPU，然后消失。这不是你希望在生产服务器上运行的东西。
